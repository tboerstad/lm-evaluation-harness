{
  "summary": {
    "total_tasks": 819,
    "working": 475,
    "working_percentage": 58.0,
    "config_load_failures": 0,
    "dataset_load_failures": 58,
    "instance_build_failures": 286
  },
  "config_load_failures": [],
  "dataset_load_failures_by_type": {
    "Dataset scripts are no longer supported, but found basqueGLUE.py": [
      "bec2016eu",
      "bhtc_v2",
      "epec_koref_bin",
      "qnlieu",
      "vaxx_stance",
      "wiceu"
    ],
    "Dataset scripts are no longer supported, but found unscramble.py": [
      "anagrams1",
      "anagrams2",
      "cycle_letters",
      "random_insertion",
      "reversed_words"
    ],
    "Gated/restricted dataset": [
      "cocoteros_va",
      "common_voice_en",
      "lingoly_context",
      "lingoly_nocontext"
    ],
    "Dataset scripts are no longer supported, but found JGLUE.py": [
      "ja_leaderboard_jcommonsenseqa",
      "ja_leaderboard_jnli",
      "ja_leaderboard_jsquad",
      "ja_leaderboard_marc_ja"
    ],
    "Dataset scripts are no longer supported, but found sycophancy.py": [
      "sycophancy_on_nlp_survey",
      "sycophancy_on_philpapers2020",
      "sycophancy_on_political_typology_quiz"
    ],
    "list index out of range": [
      [
        {
          "dataset_path": "super_glue",
          "dataset_name": "wsc.fixed",
          "use_prompt": "promptsource:*",
          "training_split": "train",
          "validation_split": "validation",
          "output_type": "generate_until",
          "metric_list": [
            {
              "metric": "exact_match",
              "aggregation": "mean",
              "higher_is_better": true,
              "ignore_case": true,
              "ignore_punctuation": true
            }
          ]
        },
        {
          "dataset_path": "winogrande",
          "dataset_name": "winogrande_xl",
          "use_prompt": "promptsource:*",
          "training_split": "train",
          "validation_split": "validation",
          "output_type": "generate_until",
          "metric_list": [
            {
              "metric": "exact_match",
              "aggregation": "mean",
              "higher_is_better": true,
              "ignore_case": true,
              "ignore_punctuation": true
            }
          ]
        },
        {
          "dataset_path": "super_glue",
          "dataset_name": "cb",
          "use_prompt": "promptsource:*",
          "training_split": "train",
          "validation_split": "validation",
          "output_type": "generate_until",
          "metric_list": [
            {
              "metric": "exact_match",
              "aggregation": "mean",
              "higher_is_better": true,
              "ignore_case": true,
              "ignore_punctuation": true
            }
          ]
        },
        {
          "dataset_path": "super_glue",
          "dataset_name": "rte",
          "use_prompt": "promptsource:*",
          "training_split": "train",
          "validation_split": "validation",
          "output_type": "generate_until",
          "metric_list": [
            {
              "metric": "exact_match",
              "aggregation": "mean",
              "higher_is_better": true,
              "ignore_case": true,
              "ignore_punctuation": true
            }
          ]
        },
        {
          "task": "anli_r1",
          "dataset_path": "anli",
          "use_prompt": "promptsource:*",
          "training_split": "train_r1",
          "validation_split": "dev_r1",
          "output_type": "generate_until",
          "metric_list": [
            {
              "metric": "exact_match",
              "aggregation": "mean",
              "higher_is_better": true,
              "ignore_case": true,
              "ignore_punctuation": true
            }
          ]
        },
        {
          "task": "anli_r2",
          "dataset_path": "anli",
          "use_prompt": "promptsource:*",
          "training_split": "train_r2",
          "validation_split": "dev_r2",
          "output_type": "generate_until",
          "metric_list": [
            {
              "metric": "exact_match",
              "aggregation": "mean",
              "higher_is_better": true,
              "ignore_case": true,
              "ignore_punctuation": true
            }
          ]
        },
        {
          "task": "anli_r3",
          "dataset_path": "anli",
          "use_prompt": "promptsource:*",
          "training_split": "train_r3",
          "validation_split": "dev_r3",
          "output_type": "generate_until",
          "metric_list": [
            {
              "metric": "exact_match",
              "aggregation": "mean",
              "higher_is_better": true,
              "ignore_case": true,
              "ignore_punctuation": true
            }
          ]
        },
        {
          "dataset_path": "super_glue",
          "dataset_name": "copa",
          "use_prompt": "promptsource:*",
          "training_split": "train",
          "validation_split": "validation",
          "output_type": "generate_until",
          "metric_list": [
            {
              "metric": "exact_match",
              "aggregation": "mean",
              "higher_is_better": true,
              "ignore_case": true,
              "ignore_punctuation": true
            }
          ]
        },
        {
          "dataset_path": "hellaswag",
          "use_prompt": "promptsource:*",
          "training_split": "train",
          "validation_split": "validation",
          "output_type": "generate_until",
          "metric_list": [
            {
              "metric": "exact_match",
              "aggregation": "mean",
              "higher_is_better": true,
              "ignore_case": true,
              "ignore_punctuation": true
            }
          ]
        },
        {
          "dataset_path": "super_glue",
          "dataset_name": "wic",
          "use_prompt": "promptsource:*",
          "training_split": "train",
          "validation_split": "validation",
          "output_type": "generate_until",
          "metric_list": [
            {
              "metric": "exact_match",
              "aggregation": "mean",
              "higher_is_better": true,
              "ignore_case": true,
              "ignore_punctuation": true
            }
          ]
        }
      ],
      "niah_single_1"
    ],
    "Dataset scripts are no longer supported, but found orange_sum.py": [
      "french_bench_orangesum_abstract",
      "french_bench_orangesum_title"
    ],
    "Dataset scripts are no longer supported, but found logiqa2.py": [
      "logieval",
      "logiqa2"
    ],
    "Dataset scripts are no longer supported, but found mediqa_qa.py": [
      "mediqa_qa2019",
      "mediqa_qa2019_perplexity"
    ],
    "Dataset scripts are no longer supported, but found qasper.py": [
      "qasper_bool",
      "qasper_freeform"
    ],
    "Dataset scripts are no longer supported, but found story_cloze.py": [
      "storycloze_2016",
      "storycloze_2018"
    ],
    "Dataset scripts are no longer supported, but found iwslt2017.py": [
      "iwslt2017-ar-en",
      "iwslt2017-en-ar"
    ],
    "Dataset scripts are no longer supported, but found arithmetic.py": [
      "arithmetic_1dc"
    ],
    "To support decoding images, please install 'Pillow'.": [
      "chartqa"
    ],
    "Dataset scripts are no longer supported, but found wikitext_fr.py": [
      "french_bench_wikitext_fr"
    ],
    "Dataset scripts are no longer supported, but found hendrycks_ethics.py": [
      "ethics_cm"
    ],
    "Dataset scripts are no longer supported, but found humaneval_infilling.py": [
      "humaneval_multi_line_infilling"
    ],
    "Dataset scripts are no longer supported, but found JAQKET.py": [
      "ja_leaderboard_jaqket_v2"
    ],
    "Dataset scripts are no longer supported, but found mgsm.py": [
      "ja_leaderboard_mgsm"
    ],
    "Dataset scripts are no longer supported, but found logiqa.py": [
      "logiqa"
    ],
    "Dataset scripts are no longer supported, but found math_qa.py": [
      "mathqa"
    ],
    "Dataset scripts are no longer supported, but found mc_taco.py": [
      "mc_taco"
    ],
    "An error occurred while generating the dataset": [
      "med_text_classification_easy"
    ],
    "Dataset scripts are no longer supported, but found meddialog.py": [
      "meddialog_raw_dialogues"
    ],
    "Dataset scripts are no longer supported, but found meqsum.py": [
      "meqsum"
    ],
    "Dataset scripts are no longer supported, but found moral_stories.py": [
      "moral_stories"
    ],
    "Dataset scripts are no longer supported, but found mutual.py": [
      "mutual"
    ],
    "Dataset scripts are no longer supported, but found pile.py": [
      "pile_arxiv"
    ],
    "Dataset scripts are no longer supported, but found prost.py": [
      "prost"
    ],
    "Dataset scripts are no longer supported, but found pubmed_qa.py": [
      "pubmedqa"
    ],
    "Dataset scripts are no longer supported, but found social_i_qa.py": [
      "social_iqa"
    ],
    "Dataset scripts are no longer supported, but found wnli-es.py": [
      "wnli_es"
    ],
    "Dataset scripts are no longer supported, but found xlsum.py": [
      "xlsum_es"
    ],
    "Dataset scripts are no longer supported, but found winograd_wsc.py": [
      "wsc273"
    ]
  },
  "instance_build_failures": [
    {
      "task": "arabic_leaderboard_alghafa_mcq_exams_test_ar",
      "error": "'gold' is undefined"
    },
    {
      "task": "arabic_leaderboard_alghafa_meta_ar_dialects",
      "error": "'gold' is undefined"
    },
    {
      "task": "arabic_leaderboard_alghafa_meta_ar_msa",
      "error": "'gold' is undefined"
    },
    {
      "task": "arabic_leaderboard_alghafa_multiple_choice_facts_truefalse_balanced_task",
      "error": "'gold' is undefined"
    },
    {
      "task": "arabic_leaderboard_alghafa_multiple_choice_grounded_statement_soqal_task",
      "error": "'gold' is undefined"
    },
    {
      "task": "arabic_leaderboard_alghafa_multiple_choice_grounded_statement_xglue_mlqa_task",
      "error": "'gold' is undefined"
    },
    {
      "task": "arabic_leaderboard_alghafa_multiple_choice_rating_sentiment_no_neutral_task",
      "error": "'gold' is undefined"
    },
    {
      "task": "arabic_leaderboard_alghafa_multiple_choice_rating_sentiment_task",
      "error": "'gold' is undefined"
    },
    {
      "task": "arabic_leaderboard_alghafa_multiple_choice_sentiment_task",
      "error": "'gold' is undefined"
    },
    {
      "task": "arabic_exams",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_abstract_algebra",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_anatomy",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_astronomy",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_business_ethics",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_clinical_knowledge",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_college_biology",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_college_chemistry",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_college_computer_science",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_college_mathematics",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_college_medicine",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_college_physics",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_computer_security",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_conceptual_physics",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_econometrics",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_electrical_engineering",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_elementary_mathematics",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_formal_logic",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_global_facts",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_high_school_biology",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_high_school_chemistry",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_high_school_computer_science",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_high_school_european_history",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_high_school_geography",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_high_school_government_and_politics",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_high_school_macroeconomics",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_high_school_mathematics",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_high_school_microeconomics",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_high_school_physics",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_high_school_psychology",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_high_school_statistics",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_high_school_us_history",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_high_school_world_history",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_human_aging",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_human_sexuality",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_international_law",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_jurisprudence",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_logical_fallacies",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_machine_learning",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_management",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_marketing",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_medical_genetics",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_miscellaneous",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_moral_disputes",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_moral_scenarios",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_nutrition",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_philosophy",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_prehistory",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_professional_accounting",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_professional_law",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_professional_medicine",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_professional_psychology",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_public_relations",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_security_studies",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_sociology",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_us_foreign_policy",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_virology",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_world_religions",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_mt_arc_challenge",
      "error": "'gold' is undefined"
    },
    {
      "task": "arabic_mt_arc_easy",
      "error": "'gold' is undefined"
    },
    {
      "task": "arabic_mt_boolq",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_mt_copa",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_mt_hellaswag",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_mt_mmlu",
      "error": "'gold' is undefined"
    },
    {
      "task": "arabic_mt_openbook_qa",
      "error": "'gold' is undefined"
    },
    {
      "task": "arabic_mt_piqa",
      "error": "'gold' is undefined"
    },
    {
      "task": "arabic_mt_race",
      "error": "'gold' is undefined"
    },
    {
      "task": "arabic_mt_sciq",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_mt_toxigen",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Algeria",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Ancient_Egypt",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arab_Empire",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Architecture",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Art",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Astronomy",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Calligraphy",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Ceremony",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Clothing",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Culture",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Food",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Funeral",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Geography",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_History",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Language_Origin",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Literature",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Math",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Medicine",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Music",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Ornament",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Philosophy",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Physics_and_Chemistry",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Wedding",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Bahrain",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Comoros",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Egypt_modern",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_InfluenceFromAncientEgypt",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_InfluenceFromByzantium",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_InfluenceFromChina",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_InfluenceFromGreece",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_InfluenceFromIslam",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_InfluenceFromPersia",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_InfluenceFromRome",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Iraq",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Islam_Education",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Islam_branches_and_schools",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Islamic_law_system",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Jordan",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Kuwait",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Lebanon",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Libya",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Mauritania",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Mesopotamia_civilization",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Morocco",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Oman",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Palestine",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Qatar",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Saudi_Arabia",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Somalia",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Sudan",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Syria",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Tunisia",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_United_Arab_Emirates",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Yemen",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_communication",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_computer_and_phone",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_daily_life",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_entertainment",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_alghafa_mcq_exams_test_ar_light",
      "error": "'gold' is undefined"
    },
    {
      "task": "arabic_leaderboard_alghafa_meta_ar_dialects_light",
      "error": "'gold' is undefined"
    },
    {
      "task": "arabic_leaderboard_alghafa_meta_ar_msa_light",
      "error": "'gold' is undefined"
    },
    {
      "task": "arabic_leaderboard_alghafa_multiple_choice_facts_truefalse_balanced_task_light",
      "error": "'gold' is undefined"
    },
    {
      "task": "arabic_leaderboard_alghafa_multiple_choice_grounded_statement_soqal_task_light",
      "error": "'gold' is undefined"
    },
    {
      "task": "arabic_leaderboard_alghafa_multiple_choice_grounded_statement_xglue_mlqa_task_light",
      "error": "'gold' is undefined"
    },
    {
      "task": "arabic_leaderboard_alghafa_multiple_choice_rating_sentiment_no_neutral_task_light",
      "error": "'gold' is undefined"
    },
    {
      "task": "arabic_leaderboard_alghafa_multiple_choice_rating_sentiment_task_light",
      "error": "'gold' is undefined"
    },
    {
      "task": "arabic_leaderboard_alghafa_multiple_choice_sentiment_task_light",
      "error": "'gold' is undefined"
    },
    {
      "task": "arabic_exams_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_abstract_algebra_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_anatomy_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_astronomy_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_business_ethics_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_clinical_knowledge_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_college_biology_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_college_chemistry_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_college_computer_science_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_college_mathematics_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_college_medicine_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_college_physics_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_computer_security_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_conceptual_physics_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_econometrics_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_electrical_engineering_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_elementary_mathematics_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_formal_logic_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_global_facts_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_high_school_biology_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_high_school_chemistry_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_high_school_computer_science_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_high_school_european_history_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_high_school_geography_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_high_school_government_and_politics_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_high_school_macroeconomics_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_high_school_mathematics_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_high_school_microeconomics_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_high_school_physics_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_high_school_psychology_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_high_school_statistics_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_high_school_us_history_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_high_school_world_history_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_human_aging_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_human_sexuality_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_international_law_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_jurisprudence_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_logical_fallacies_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_machine_learning_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_management_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_marketing_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_medical_genetics_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_miscellaneous_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_moral_disputes_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_moral_scenarios_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_nutrition_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_philosophy_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_prehistory_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_professional_accounting_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_professional_law_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_professional_medicine_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_professional_psychology_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_public_relations_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_security_studies_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_sociology_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_us_foreign_policy_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_virology_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_arabic_mmlu_world_religions_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_mt_arc_challenge_light",
      "error": "'gold' is undefined"
    },
    {
      "task": "arabic_mt_arc_easy_light",
      "error": "'gold' is undefined"
    },
    {
      "task": "arabic_mt_boolq_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_mt_copa_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_mt_hellaswag_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_mt_mmlu_light",
      "error": "'gold' is undefined"
    },
    {
      "task": "arabic_mt_openbook_qa_light",
      "error": "'gold' is undefined"
    },
    {
      "task": "arabic_mt_piqa_light",
      "error": "'gold' is undefined"
    },
    {
      "task": "arabic_mt_race_light",
      "error": "'gold' is undefined"
    },
    {
      "task": "arabic_mt_sciq_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_mt_toxigen_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Algeria_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Ancient_Egypt_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arab_Empire_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Architecture_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Art_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Astronomy_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Calligraphy_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Ceremony_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Clothing_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Culture_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Food_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Funeral_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Geography_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_History_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Language_Origin_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Literature_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Math_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Medicine_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Music_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Ornament_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Philosophy_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Physics_and_Chemistry_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Arabic_Wedding_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Bahrain_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Comoros_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Egypt_modern_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_InfluenceFromAncientEgypt_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_InfluenceFromByzantium_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_InfluenceFromChina_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_InfluenceFromGreece_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_InfluenceFromIslam_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_InfluenceFromPersia_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_InfluenceFromRome_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Iraq_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Islam_Education_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Islam_branches_and_schools_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Islamic_law_system_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Jordan_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Kuwait_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Lebanon_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Libya_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Mauritania_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Mesopotamia_civilization_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Morocco_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Oman_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Palestine_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Qatar_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Saudi_Arabia_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Somalia_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Sudan_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Syria_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Tunisia_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_United_Arab_Emirates_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_Yemen_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_communication_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_computer_and_phone_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_daily_life_light",
      "error": "'query' is undefined"
    },
    {
      "task": "arabic_leaderboard_acva_entertainment_light",
      "error": "'query' is undefined"
    },
    {
      "task": "darijahellaswag",
      "error": "'query' is undefined"
    },
    {
      "task": "egyhellaswag",
      "error": "'query' is undefined"
    },
    {
      "task": "french_bench_hellaswag",
      "error": "'query' is undefined"
    },
    {
      "task": "groundcocoa",
      "error": "'criteria' is undefined"
    },
    {
      "task": "hellaswag",
      "error": "'query' is undefined"
    },
    {
      "task": "hendrycks_math_algebra",
      "error": "'answer' is undefined"
    },
    {
      "task": "histoires_morales",
      "error": "'query' is undefined"
    },
    {
      "task": "kobest_hellaswag",
      "error": "'query' is undefined"
    },
    {
      "task": "metabench_arc",
      "error": "'twentyfive_shot_preprompt' is undefined"
    },
    {
      "task": "metabench_gsm8k",
      "error": "'five_shot_preprompt' is undefined"
    },
    {
      "task": "metabench_hellaswag",
      "error": "'ten_shot_preprompt' is undefined"
    },
    {
      "task": "metabench_mmlu",
      "error": "'five_shot_preprompt' is undefined"
    },
    {
      "task": "minerva_math_algebra",
      "error": "'answer' is undefined"
    },
    {
      "task": "tinyHellaswag",
      "error": "'query' is undefined"
    }
  ],
  "working_tasks": [
    "agieval_aqua_rat",
    "agieval_gaokao_biology",
    "agieval_gaokao_chemistry",
    "agieval_gaokao_chinese",
    "agieval_gaokao_english",
    "agieval_gaokao_geography",
    "agieval_gaokao_history",
    "agieval_gaokao_mathcloze",
    "agieval_gaokao_mathqa",
    "agieval_gaokao_physics",
    "agieval_jec_qa_ca",
    "agieval_jec_qa_kd",
    "agieval_logiqa_en",
    "agieval_logiqa_zh",
    "agieval_lsat_ar",
    "agieval_lsat_lr",
    "agieval_lsat_rc",
    "agieval_math",
    "agieval_sat_en_without_passage",
    "agieval_sat_en",
    "agieval_sat_math",
    "aime",
    "aime24",
    "aime25",
    "copa_ar",
    "piqa_ar",
    "anli_r1",
    "AraDiCE_boolq_egy",
    "AraDiCE_boolq_eng",
    "AraDiCE_boolq_lev",
    "AraDiCE_boolq_msa",
    "AraDiCE_egypt_cultural",
    "AraDiCE_jordan_cultural",
    "AraDiCE_lebanon_cultural",
    "AraDiCE_palestine_cultural",
    "AraDiCE_qatar_cultural",
    "AraDiCE_syria_cultural",
    "AraDiCE_openbookqa_egy",
    "AraDiCE_openbookqa_eng",
    "AraDiCE_openbookqa_lev",
    "AraDiCE_openbookqa_msa",
    "AraDiCE_piqa_egy",
    "AraDiCE_piqa_eng",
    "AraDiCE_piqa_lev",
    "AraDiCE_piqa_msa",
    "AraDiCE_truthfulqa_mc1_egy",
    "AraDiCE_truthfulqa_mc1_eng",
    "AraDiCE_truthfulqa_mc1_lev",
    "AraDiCE_truthfulqa_mc1_msa",
    "AraDiCE_winogrande_egy",
    "AraDiCE_winogrande_eng",
    "AraDiCE_winogrande_lev",
    "AraDiCE_winogrande_msa",
    "arc_challenge_chat",
    "arc_easy",
    "arc_challenge_mt_fi",
    "arc_challenge_mt_is",
    "asdiv_cot_llama",
    "asdiv",
    "babi",
    "bangla_mmlu",
    "arc_eu_easy",
    "mgsm_native_cot_eu",
    "mgsm_direct_eu",
    "paws_eu",
    "piqa_eu",
    "wnli_eu",
    "xcopa_eu",
    "bbq_generate",
    "bbq",
    "c4",
    "careqa_en",
    "careqa_open",
    "catalanqa",
    "catcola",
    "copa_ca",
    "coqcat",
    "mgsm_direct_ca",
    "openbookqa_ca",
    "parafraseja",
    "paws_ca",
    "piqa_ca",
    "siqa_ca",
    "teca",
    "wnli_ca",
    "xnli_ca",
    "xnli_va",
    "xquad_ca",
    "xstorycloze_ca",
    "cnn_dailymail_abisee",
    "code2text_go",
    "code2text_java",
    "code2text_javascript",
    "code2text_php",
    "code2text_python",
    "code2text_ruby",
    "commonsense_qa",
    "copal_id_standard",
    "coqa",
    "crows_pairs_english",
    "discrim_eval_explicit",
    "discrim_eval_implicit",
    "drop",
    "mmlu_early_training",
    "eq_bench",
    "eqbench_ca",
    "eqbench_es",
    "eus_proficiency",
    "eus_reading",
    "eus_trivia",
    "fld_default",
    "fld_logical_formula_default",
    "french_bench_arc_challenge",
    "french_bench_boolqa",
    "french_bench_fquadv2",
    "french_bench_fquadv2_bool",
    "french_bench_fquadv2_genq",
    "french_bench_fquadv2_hasAns",
    "french_bench_grammar",
    "french_bench_multifquad",
    "french_bench_opus_perplexity",
    "french_bench_reading_comp",
    "french_bench_topic_based_nli",
    "french_bench_trivia",
    "french_bench_vocab",
    "french_bench_xnli",
    "belebele_glg_Latn",
    "galcola",
    "mgsm_direct_gl",
    "openbookqa_gl",
    "parafrases_gl",
    "paws_gl",
    "summarization_gl",
    "truthfulqa_gl_gen",
    "truthfulqa_gl_mc1",
    "xnli_gl",
    "xstorycloze_gl",
    "glianorex",
    "glianorex_en",
    "glianorex_fr",
    "cola",
    "mnli",
    "mrpc",
    "qnli",
    "qqp",
    "rte",
    "sst2",
    "wnli",
    "gsm8k_cot_llama",
    "gsm8k_cot_zeroshot",
    "gsm8k_cot",
    "gsm8k",
    "gsm8k_platinum_cot_llama",
    "gsm8k_platinum_cot_zeroshot",
    "gsm8k_platinum_cot",
    "gsm8k_platinum",
    "gsm_plus",
    "gsm_plus_mini",
    "headqa_en",
    "hendrycks_math500",
    "humaneval",
    "humaneval_plus",
    "icelandic_winogrande",
    "ifeval",
    "inverse_scaling_hindsight_neglect_10shot",
    "inverse_scaling_into_the_unknown",
    "inverse_scaling_memo_trap",
    "inverse_scaling_modus_tollens",
    "inverse_scaling_neqa",
    "inverse_scaling_pattern_matching_suppression",
    "inverse_scaling_quote_repetition",
    "inverse_scaling_redefine_math",
    "inverse_scaling_repetitive_algebra",
    "inverse_scaling_sig_figs",
    "inverse_scaling_winobias_antistereotype",
    "ja_leaderboard_xlsum",
    "ja_leaderboard_xwinograd",
    "jsonschema_bench_easy",
    "kobest_boolq",
    "kobest_copa",
    "kobest_sentineg",
    "kobest_wic",
    "lambada_openai",
    "lambada_standard",
    "lambada_openai_cloze_yaml",
    "lambada_standard_cloze_yaml",
    "lambada_openai_mt_en",
    "lambada_openai_mt_stablelm_en",
    "leaderboard_ifeval",
    "leaderboard_mmlu_pro",
    "arc_challenge_llama",
    "longbench_2wikimqa",
    "longbench_2wikimqa_e",
    "longbench_dureader",
    "longbench_gov_report",
    "longbench_gov_report_e",
    "longbench_hotpotqa",
    "longbench_hotpotqa_e",
    "longbench_lcc",
    "longbench_lcc_e",
    "longbench_lsht",
    "longbench_multi_news",
    "longbench_multi_news_e",
    "longbench_multifieldqa_en",
    "longbench_multifieldqa_en_e",
    "longbench_multifieldqa_zh",
    "longbench_musique",
    "longbench_narrativeqa",
    "longbench_passage_count",
    "longbench_passage_count_e",
    "longbench_passage_retrieval_en",
    "longbench_passage_retrieval_en_e",
    "longbench_passage_retrieval_zh",
    "longbench_qasper",
    "longbench_qasper_e",
    "longbench_qmsum",
    "longbench_repobench-p",
    "longbench_repobench-p_e",
    "longbench_samsum",
    "longbench_samsum_e",
    "longbench_trec",
    "longbench_trec_e",
    "longbench_triviaqa",
    "longbench_triviaqa_e",
    "longbench_vcsum",
    "mastermind_24_easy",
    "mastermind_24_hard",
    "mastermind_35_easy",
    "mastermind_35_hard",
    "mastermind_46_easy",
    "mastermind_46_hard",
    "mbpp",
    "mbpp_instruct",
    "mbpp_plus",
    "mbpp_plus_instruct",
    "med_prescriptions_easy",
    "meddialog_qsumm",
    "medmcqa",
    "medqa_4options",
    "medtext",
    "mela_en",
    "metabench_truthfulqa",
    "metabench_winogrande",
    "mimic_repsum",
    "minerva_math500",
    "mmlu_redux_spanish",
    "mts_dialog",
    "ncb",
    "norec_document_p0",
    "norec_document_p1",
    "norec_document_p2",
    "norec_document_p3",
    "norec_document_p4",
    "norec_sentence_p0",
    "norec_sentence_p1",
    "norec_sentence_p2",
    "norec_sentence_p3",
    "norec_sentence_p4",
    "norrewrite_instruct",
    "norsummarize_instruct",
    "noticia",
    "nq_open",
    "arc_ar",
    "arc_bn",
    "arc_ca",
    "arc_da",
    "arc_de",
    "arc_es",
    "arc_eu",
    "arc_fr",
    "arc_gu",
    "arc_hi",
    "arc_hr",
    "arc_hu",
    "arc_hy",
    "arc_id",
    "arc_it",
    "arc_kn",
    "arc_ml",
    "arc_mr",
    "arc_ne",
    "arc_nl",
    "arc_pt",
    "arc_ro",
    "arc_ru",
    "arc_sk",
    "arc_sr",
    "arc_sv",
    "arc_ta",
    "arc_te",
    "arc_uk",
    "arc_vi",
    "arc_zh",
    "hellaswag_ar",
    "hellaswag_bn",
    "hellaswag_ca",
    "hellaswag_da",
    "hellaswag_de",
    "hellaswag_es",
    "hellaswag_eu",
    "hellaswag_fr",
    "hellaswag_gu",
    "hellaswag_hi",
    "hellaswag_hr",
    "hellaswag_hu",
    "hellaswag_hy",
    "hellaswag_id",
    "hellaswag_it",
    "hellaswag_kn",
    "hellaswag_ml",
    "hellaswag_mr",
    "hellaswag_ne",
    "hellaswag_nl",
    "hellaswag_pt",
    "hellaswag_ro",
    "hellaswag_ru",
    "hellaswag_sk",
    "hellaswag_sr",
    "hellaswag_sv",
    "hellaswag_ta",
    "hellaswag_te",
    "hellaswag_uk",
    "hellaswag_vi",
    "truthfulqa_ar_mc1",
    "truthfulqa_ar_mc2",
    "truthfulqa_bn_mc1",
    "truthfulqa_bn_mc2",
    "truthfulqa_ca_mc1",
    "truthfulqa_ca_mc2",
    "truthfulqa_da_mc1",
    "truthfulqa_da_mc2",
    "truthfulqa_de_mc1",
    "truthfulqa_de_mc2",
    "truthfulqa_es_mc1",
    "truthfulqa_es_mc2",
    "truthfulqa_eu_mc1",
    "truthfulqa_eu_mc2",
    "truthfulqa_fr_mc1",
    "truthfulqa_fr_mc2",
    "truthfulqa_gu_mc1",
    "truthfulqa_gu_mc2",
    "truthfulqa_hi_mc1",
    "truthfulqa_hi_mc2",
    "truthfulqa_hr_mc1",
    "truthfulqa_hr_mc2",
    "truthfulqa_hu_mc1",
    "truthfulqa_hu_mc2",
    "truthfulqa_hy_mc1",
    "truthfulqa_hy_mc2",
    "truthfulqa_id_mc1",
    "truthfulqa_id_mc2",
    "truthfulqa_it_mc1",
    "truthfulqa_it_mc2",
    "truthfulqa_kn_mc1",
    "truthfulqa_kn_mc2",
    "truthfulqa_ml_mc1",
    "truthfulqa_ml_mc2",
    "truthfulqa_mr_mc1",
    "truthfulqa_mr_mc2",
    "truthfulqa_ne_mc1",
    "truthfulqa_ne_mc2",
    "truthfulqa_nl_mc1",
    "truthfulqa_nl_mc2",
    "truthfulqa_pt_mc1",
    "truthfulqa_pt_mc2",
    "truthfulqa_ro_mc1",
    "truthfulqa_ro_mc2",
    "truthfulqa_ru_mc1",
    "truthfulqa_ru_mc2",
    "truthfulqa_sk_mc1",
    "truthfulqa_sk_mc2",
    "truthfulqa_sr_mc1",
    "truthfulqa_sr_mc2",
    "truthfulqa_sv_mc1",
    "truthfulqa_sv_mc2",
    "truthfulqa_ta_mc1",
    "truthfulqa_ta_mc2",
    "truthfulqa_te_mc1",
    "truthfulqa_te_mc2",
    "truthfulqa_uk_mc1",
    "truthfulqa_uk_mc2",
    "truthfulqa_vi_mc1",
    "truthfulqa_vi_mc2",
    "truthfulqa_zh_mc1",
    "truthfulqa_zh_mc2",
    "olaph",
    "olaph_perplexity",
    "openbookqa",
    "pile_10k",
    "piqa",
    "polemo2_in",
    "polemo2_out",
    "assin_entailment",
    "assin_paraphrase",
    "qa4mre_2011",
    "qa4mre_2012",
    "qa4mre_2013",
    "race",
    "realtoxicityprompts",
    "sciq",
    "non_greedy_robustness_agieval_aqua_rat",
    "non_greedy_robustness_agieval_logiqa_en",
    "non_greedy_robustness_agieval_lsat_rc",
    "non_greedy_robustness_agieval_lsat_ar",
    "non_greedy_robustness_agieval_lsat_lr",
    "non_greedy_robustness_agieval_sat_en",
    "non_greedy_robustness_agieval_sat_math",
    "option_order_robustness_agieval_aqua_rat",
    "option_order_robustness_agieval_logiqa_en",
    "option_order_robustness_agieval_lsat_ar",
    "option_order_robustness_agieval_lsat_lr",
    "option_order_robustness_agieval_lsat_rc",
    "option_order_robustness_agieval_sat_en",
    "option_order_robustness_agieval_sat_math",
    "prompt_robustness_agieval_aqua_rat",
    "prompt_robustness_agieval_logiqa_en",
    "prompt_robustness_agieval_lsat_rc",
    "prompt_robustness_agieval_lsat_ar",
    "prompt_robustness_agieval_lsat_lr",
    "prompt_robustness_agieval_sat_en",
    "prompt_robustness_agieval_sat_math",
    "non_greedy_robustness_math_algebra",
    "prompt_robustness_math_algebra",
    "score_non_greedy_robustness_mmlu_pro",
    "score_option_order_robustness_mmlu_pro",
    "score_prompt_robustness_mmlu_pro",
    "simple_cooccurrence_bias",
    "simple_cooccurrence_bias_gen",
    "cocoteros_es",
    "copa_es",
    "escola",
    "openbookqa_es",
    "paws_es_spanish_bench",
    "xnli_es_spanish_bench",
    "boolq",
    "boolq-seq2seq",
    "super_glue-boolq-t5-prompt",
    "cb",
    "super_glue-cb-t5-prompt",
    "copa",
    "super_glue-copa-t5-prompt",
    "multirc",
    "super_glue-multirc-t5-prompt",
    "record",
    "super_glue-record-t5-prompt",
    "sglue_rte",
    "super_glue-rte-t5-prompt",
    "wic",
    "super_glue-wic-t5-prompt",
    "wsc",
    "super_glue-wsc-t5-prompt",
    "swag",
    "tinyArc",
    "tinyGSM8k",
    "tinyMMLU",
    "tinyTruthfulQA_mc1",
    "tinyWinogrande",
    "toxigen",
    "wmt14-en-fr",
    "wmt14-fr-en",
    "wmt16-de-en",
    "wmt16-en-de",
    "wmt16-en-ro",
    "wmt16-ro-en",
    "triviaqa",
    "truthfulqa_gen",
    "truthfulqa_mc1",
    "webqs",
    "wikitext",
    "winogender_all",
    "winogrande",
    "wmt-ro-en-t5-prompt",
    "xcopa_et",
    "xnli_eu",
    "xstorycloze_ar"
  ],
  "detailed_results": [
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/agieval/aqua-rat.yaml",
      "task_name": "agieval_aqua_rat",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-aqua-rat",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.9
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/agieval/gaokao-biology.yaml",
      "task_name": "agieval_gaokao_biology",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-gaokao-biology",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.05
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/agieval/gaokao-chemistry.yaml",
      "task_name": "agieval_gaokao_chemistry",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-gaokao-chemistry",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.76
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/agieval/gaokao-chinese.yaml",
      "task_name": "agieval_gaokao_chinese",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-gaokao-chinese",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.9
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/agieval/gaokao-english.yaml",
      "task_name": "agieval_gaokao_english",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-gaokao-english",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.18
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/agieval/gaokao-geography.yaml",
      "task_name": "agieval_gaokao_geography",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-gaokao-geography",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.21
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/agieval/gaokao-history.yaml",
      "task_name": "agieval_gaokao_history",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-gaokao-history",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.75
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/agieval/gaokao-mathcloze.yaml",
      "task_name": "agieval_gaokao_mathcloze",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-gaokao-mathcloze",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.31
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/agieval/gaokao-mathqa.yaml",
      "task_name": "agieval_gaokao_mathqa",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-gaokao-mathqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.0
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/agieval/gaokao-physics.yaml",
      "task_name": "agieval_gaokao_physics",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-gaokao-physics",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.17
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/agieval/jec-qa-ca.yaml",
      "task_name": "agieval_jec_qa_ca",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-jec-qa-ca",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.93
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/agieval/jec-qa-kd.yaml",
      "task_name": "agieval_jec_qa_kd",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-jec-qa-kd",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.74
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/agieval/logiqa-en.yaml",
      "task_name": "agieval_logiqa_en",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-logiqa-en",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.2
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/agieval/logiqa-zh.yaml",
      "task_name": "agieval_logiqa_zh",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-logiqa-zh",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.74
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/agieval/lsat-ar.yaml",
      "task_name": "agieval_lsat_ar",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-lsat-ar",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.77
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/agieval/lsat-lr.yaml",
      "task_name": "agieval_lsat_lr",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-lsat-lr",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.97
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/agieval/lsat-rc.yaml",
      "task_name": "agieval_lsat_rc",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-lsat-rc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.02
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/agieval/math.yaml",
      "task_name": "agieval_math",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-math",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.98
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/agieval/sat-en-without-passage.yaml",
      "task_name": "agieval_sat_en_without_passage",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-sat-en-without-passage",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.84
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/agieval/sat-en.yaml",
      "task_name": "agieval_sat_en",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-sat-en",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.04
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/agieval/sat-math.yaml",
      "task_name": "agieval_sat_math",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-sat-math",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.19
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/aime/aime.yaml",
      "task_name": "aime",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "gneubig/aime-1983-2024",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.93
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/aime/aime24.yaml",
      "task_name": "aime24",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Maxwell-Jia/AIME_2024",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.92
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/aime/aime25.yaml",
      "task_name": "aime25",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "math-ai/aime25",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.88
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/alghafa/copa_ar/copa_ar.yaml",
      "task_name": "copa_ar",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Hennara/copa_ar",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.97
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/alghafa/piqa_ar/piqa_ar.yaml",
      "task_name": "piqa_ar",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Hennara/pica_ar",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.06
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/anli/anli_r1.yaml",
      "task_name": "anli_r1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "anli",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 6.93
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_alghafa/arabic_leaderboard_alghafa_mcq_exams_test_ar.yaml",
      "task_name": "arabic_leaderboard_alghafa_mcq_exams_test_ar",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "dataset_load_error": "",
      "instance_build_error": "'gold' is undefined",
      "test_time_seconds": 1.17
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_alghafa/arabic_leaderboard_alghafa_meta_ar_dialects.yaml",
      "task_name": "arabic_leaderboard_alghafa_meta_ar_dialects",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "dataset_load_error": "",
      "instance_build_error": "'gold' is undefined",
      "test_time_seconds": 0.85
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_alghafa/arabic_leaderboard_alghafa_meta_ar_msa.yaml",
      "task_name": "arabic_leaderboard_alghafa_meta_ar_msa",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "dataset_load_error": "",
      "instance_build_error": "'gold' is undefined",
      "test_time_seconds": 0.66
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_alghafa/arabic_leaderboard_alghafa_multiple_choice_facts_truefalse_balanced_task.yaml",
      "task_name": "arabic_leaderboard_alghafa_multiple_choice_facts_truefalse_balanced_task",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "dataset_load_error": "",
      "instance_build_error": "'gold' is undefined",
      "test_time_seconds": 0.61
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_alghafa/arabic_leaderboard_alghafa_multiple_choice_grounded_statement_soqal_task.yaml",
      "task_name": "arabic_leaderboard_alghafa_multiple_choice_grounded_statement_soqal_task",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "dataset_load_error": "",
      "instance_build_error": "'gold' is undefined",
      "test_time_seconds": 0.67
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_alghafa/arabic_leaderboard_alghafa_multiple_choice_grounded_statement_xglue_mlqa_task.yaml",
      "task_name": "arabic_leaderboard_alghafa_multiple_choice_grounded_statement_xglue_mlqa_task",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "dataset_load_error": "",
      "instance_build_error": "'gold' is undefined",
      "test_time_seconds": 0.6
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_alghafa/arabic_leaderboard_alghafa_multiple_choice_rating_sentiment_no_neutral_task.yaml",
      "task_name": "arabic_leaderboard_alghafa_multiple_choice_rating_sentiment_no_neutral_task",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "dataset_load_error": "",
      "instance_build_error": "'gold' is undefined",
      "test_time_seconds": 0.65
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_alghafa/arabic_leaderboard_alghafa_multiple_choice_rating_sentiment_task.yaml",
      "task_name": "arabic_leaderboard_alghafa_multiple_choice_rating_sentiment_task",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "dataset_load_error": "",
      "instance_build_error": "'gold' is undefined",
      "test_time_seconds": 1.53
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_alghafa/arabic_leaderboard_alghafa_multiple_choice_sentiment_task.yaml",
      "task_name": "arabic_leaderboard_alghafa_multiple_choice_sentiment_task",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "dataset_load_error": "",
      "instance_build_error": "'gold' is undefined",
      "test_time_seconds": 0.69
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_exams/arabic_exams.yaml",
      "task_name": "arabic_exams",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_EXAMS",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 1.22
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_abstract_algebra.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_abstract_algebra",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.92
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_anatomy.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_anatomy",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.83
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_astronomy.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_astronomy",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.82
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_business_ethics.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_business_ethics",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.95
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_clinical_knowledge.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_clinical_knowledge",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.78
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_college_biology.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_college_biology",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.77
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_college_chemistry.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_college_chemistry",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.78
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_college_computer_science.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_college_computer_science",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.77
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_college_mathematics.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_college_mathematics",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.61
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_college_medicine.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_college_medicine",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.65
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_college_physics.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_college_physics",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.68
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_computer_security.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_computer_security",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.75
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_conceptual_physics.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_conceptual_physics",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.66
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_econometrics.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_econometrics",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.88
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_electrical_engineering.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_electrical_engineering",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 12.78
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_elementary_mathematics.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_elementary_mathematics",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.77
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_formal_logic.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_formal_logic",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.64
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_global_facts.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_global_facts",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.66
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_high_school_biology.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_high_school_biology",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.79
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_high_school_chemistry.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_high_school_chemistry",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.72
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_high_school_computer_science.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_high_school_computer_science",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.76
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_high_school_european_history.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_high_school_european_history",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.66
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_high_school_geography.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_high_school_geography",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.76
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_high_school_government_and_politics.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_high_school_government_and_politics",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.64
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_high_school_macroeconomics.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_high_school_macroeconomics",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.75
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_high_school_mathematics.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_high_school_mathematics",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.7
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_high_school_microeconomics.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_high_school_microeconomics",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.62
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_high_school_physics.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_high_school_physics",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.79
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_high_school_psychology.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_high_school_psychology",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 1.01
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_high_school_statistics.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_high_school_statistics",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.73
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_high_school_us_history.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_high_school_us_history",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.89
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_high_school_world_history.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_high_school_world_history",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.65
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_human_aging.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_human_aging",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.7
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_human_sexuality.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_human_sexuality",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.75
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_international_law.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_international_law",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.64
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_jurisprudence.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_jurisprudence",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.72
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_logical_fallacies.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_logical_fallacies",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.66
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_machine_learning.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_machine_learning",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.88
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_management.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_management",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 1.02
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_marketing.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_marketing",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.64
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_medical_genetics.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_medical_genetics",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.62
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_miscellaneous.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_miscellaneous",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.74
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_moral_disputes.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_moral_disputes",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.85
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_moral_scenarios.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_moral_scenarios",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.81
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_nutrition.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_nutrition",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.89
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_philosophy.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_philosophy",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.63
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_prehistory.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_prehistory",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.73
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_professional_accounting.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_professional_accounting",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.78
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_professional_law.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_professional_law",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.85
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_professional_medicine.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_professional_medicine",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.74
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_professional_psychology.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_professional_psychology",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.7
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_public_relations.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_public_relations",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.77
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_security_studies.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_security_studies",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 6.64
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_sociology.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_sociology",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.84
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_us_foreign_policy.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_us_foreign_policy",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.74
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_virology.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_virology",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 1.52
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mmlu/arabic_leaderboard_arabic_mmlu_world_religions.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_world_religions",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/Arabic_MMLU",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.71
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mt_arc_challenge/arabic_mt_arc_challenge.yaml",
      "task_name": "arabic_mt_arc_challenge",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "dataset_load_error": "",
      "instance_build_error": "'gold' is undefined",
      "test_time_seconds": 8.32
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mt_arc_easy/arabic_mt_arc_easy.yaml",
      "task_name": "arabic_mt_arc_easy",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "dataset_load_error": "",
      "instance_build_error": "'gold' is undefined",
      "test_time_seconds": 0.76
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mt_boolq/arabic_mt_boolq.yaml",
      "task_name": "arabic_mt_boolq",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.67
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mt_copa/arabic_mt_copa.yaml",
      "task_name": "arabic_mt_copa",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.84
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mt_hellaswag/arabic_mt_hellaswag.yaml",
      "task_name": "arabic_mt_hellaswag",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.71
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mt_mmlu/arabic_mt_mmlu.yaml",
      "task_name": "arabic_mt_mmlu",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "dataset_load_error": "",
      "instance_build_error": "'gold' is undefined",
      "test_time_seconds": 0.63
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mt_openbook_qa/arabic_mt_openbook_qa.yaml",
      "task_name": "arabic_mt_openbook_qa",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "dataset_load_error": "",
      "instance_build_error": "'gold' is undefined",
      "test_time_seconds": 0.61
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mt_piqa/arabic_mt_piqa.yaml",
      "task_name": "arabic_mt_piqa",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "dataset_load_error": "",
      "instance_build_error": "'gold' is undefined",
      "test_time_seconds": 0.69
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mt_race/arabic_mt_race.yaml",
      "task_name": "arabic_mt_race",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "dataset_load_error": "",
      "instance_build_error": "'gold' is undefined",
      "test_time_seconds": 0.67
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mt_sciq/arabic_mt_sciq.yaml",
      "task_name": "arabic_mt_sciq",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.78
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_arabic_mt_toxigen/arabic_mt_toxigen.yaml",
      "task_name": "arabic_mt_toxigen",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.68
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Algeria.yaml",
      "task_name": "arabic_leaderboard_acva_Algeria",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.84
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Ancient_Egypt.yaml",
      "task_name": "arabic_leaderboard_acva_Ancient_Egypt",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.7
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Arab_Empire.yaml",
      "task_name": "arabic_leaderboard_acva_Arab_Empire",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.67
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Arabic_Architecture.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Architecture",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.6
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Arabic_Art.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Art",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.67
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Arabic_Astronomy.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Astronomy",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.64
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Arabic_Calligraphy.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Calligraphy",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.64
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Arabic_Ceremony.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Ceremony",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.78
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Arabic_Clothing.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Clothing",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.61
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Arabic_Culture.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Culture",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.59
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Arabic_Food.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Food",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.59
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Arabic_Funeral.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Funeral",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.91
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Arabic_Geography.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Geography",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.61
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Arabic_History.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_History",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.59
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Arabic_Language_Origin.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Language_Origin",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.63
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Arabic_Literature.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Literature",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.8
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Arabic_Math.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Math",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.65
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Arabic_Medicine.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Medicine",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.59
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Arabic_Music.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Music",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.68
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Arabic_Ornament.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Ornament",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.58
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Arabic_Philosophy.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Philosophy",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.62
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Arabic_Physics_and_Chemistry.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Physics_and_Chemistry",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.69
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Arabic_Wedding.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Wedding",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.59
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Bahrain.yaml",
      "task_name": "arabic_leaderboard_acva_Bahrain",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.59
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Comoros.yaml",
      "task_name": "arabic_leaderboard_acva_Comoros",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.64
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Egypt_modern.yaml",
      "task_name": "arabic_leaderboard_acva_Egypt_modern",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 13.6
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_InfluenceFromAncientEgypt.yaml",
      "task_name": "arabic_leaderboard_acva_InfluenceFromAncientEgypt",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.63
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_InfluenceFromByzantium.yaml",
      "task_name": "arabic_leaderboard_acva_InfluenceFromByzantium",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.71
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_InfluenceFromChina.yaml",
      "task_name": "arabic_leaderboard_acva_InfluenceFromChina",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.69
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_InfluenceFromGreece.yaml",
      "task_name": "arabic_leaderboard_acva_InfluenceFromGreece",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.64
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_InfluenceFromIslam.yaml",
      "task_name": "arabic_leaderboard_acva_InfluenceFromIslam",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.7
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_InfluenceFromPersia.yaml",
      "task_name": "arabic_leaderboard_acva_InfluenceFromPersia",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.63
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_InfluenceFromRome.yaml",
      "task_name": "arabic_leaderboard_acva_InfluenceFromRome",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.82
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Iraq.yaml",
      "task_name": "arabic_leaderboard_acva_Iraq",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.65
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Islam_Education.yaml",
      "task_name": "arabic_leaderboard_acva_Islam_Education",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.63
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Islam_branches_and_schools.yaml",
      "task_name": "arabic_leaderboard_acva_Islam_branches_and_schools",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.61
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Islamic_law_system.yaml",
      "task_name": "arabic_leaderboard_acva_Islamic_law_system",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.66
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Jordan.yaml",
      "task_name": "arabic_leaderboard_acva_Jordan",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.66
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Kuwait.yaml",
      "task_name": "arabic_leaderboard_acva_Kuwait",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.7
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Lebanon.yaml",
      "task_name": "arabic_leaderboard_acva_Lebanon",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.61
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Libya.yaml",
      "task_name": "arabic_leaderboard_acva_Libya",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.62
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Mauritania.yaml",
      "task_name": "arabic_leaderboard_acva_Mauritania",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.98
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Mesopotamia_civilization.yaml",
      "task_name": "arabic_leaderboard_acva_Mesopotamia_civilization",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.67
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Morocco.yaml",
      "task_name": "arabic_leaderboard_acva_Morocco",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.65
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Oman.yaml",
      "task_name": "arabic_leaderboard_acva_Oman",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.61
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Palestine.yaml",
      "task_name": "arabic_leaderboard_acva_Palestine",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.62
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Qatar.yaml",
      "task_name": "arabic_leaderboard_acva_Qatar",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.62
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Saudi_Arabia.yaml",
      "task_name": "arabic_leaderboard_acva_Saudi_Arabia",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.76
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Somalia.yaml",
      "task_name": "arabic_leaderboard_acva_Somalia",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.66
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Sudan.yaml",
      "task_name": "arabic_leaderboard_acva_Sudan",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.63
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Syria.yaml",
      "task_name": "arabic_leaderboard_acva_Syria",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.7
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Tunisia.yaml",
      "task_name": "arabic_leaderboard_acva_Tunisia",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.68
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_United_Arab_Emirates.yaml",
      "task_name": "arabic_leaderboard_acva_United_Arab_Emirates",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.63
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_Yemen.yaml",
      "task_name": "arabic_leaderboard_acva_Yemen",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.64
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_communication.yaml",
      "task_name": "arabic_leaderboard_acva_communication",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.63
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_computer_and_phone.yaml",
      "task_name": "arabic_leaderboard_acva_computer_and_phone",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.63
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_daily_life.yaml",
      "task_name": "arabic_leaderboard_acva_daily_life",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.63
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_complete/arabic_leaderboard_avca/arabic_leaderboard_acva_entertainment.yaml",
      "task_name": "arabic_leaderboard_acva_entertainment",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.75
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_alghafa_light/arabic_leaderboard_alghafa_mcq_exams_test_ar_light.yaml",
      "task_name": "arabic_leaderboard_alghafa_mcq_exams_test_ar_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/AlGhafa-Arabic-LLM-Benchmark-Native-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'gold' is undefined",
      "test_time_seconds": 0.8
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_alghafa_light/arabic_leaderboard_alghafa_meta_ar_dialects_light.yaml",
      "task_name": "arabic_leaderboard_alghafa_meta_ar_dialects_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/AlGhafa-Arabic-LLM-Benchmark-Native-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'gold' is undefined",
      "test_time_seconds": 0.75
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_alghafa_light/arabic_leaderboard_alghafa_meta_ar_msa_light.yaml",
      "task_name": "arabic_leaderboard_alghafa_meta_ar_msa_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/AlGhafa-Arabic-LLM-Benchmark-Native-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'gold' is undefined",
      "test_time_seconds": 0.6
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_alghafa_light/arabic_leaderboard_alghafa_multiple_choice_facts_truefalse_balanced_task_light.yaml",
      "task_name": "arabic_leaderboard_alghafa_multiple_choice_facts_truefalse_balanced_task_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/AlGhafa-Arabic-LLM-Benchmark-Native-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'gold' is undefined",
      "test_time_seconds": 0.55
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_alghafa_light/arabic_leaderboard_alghafa_multiple_choice_grounded_statement_soqal_task_light.yaml",
      "task_name": "arabic_leaderboard_alghafa_multiple_choice_grounded_statement_soqal_task_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/AlGhafa-Arabic-LLM-Benchmark-Native-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'gold' is undefined",
      "test_time_seconds": 0.55
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_alghafa_light/arabic_leaderboard_alghafa_multiple_choice_grounded_statement_xglue_mlqa_task_light.yaml",
      "task_name": "arabic_leaderboard_alghafa_multiple_choice_grounded_statement_xglue_mlqa_task_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/AlGhafa-Arabic-LLM-Benchmark-Native-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'gold' is undefined",
      "test_time_seconds": 0.65
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_alghafa_light/arabic_leaderboard_alghafa_multiple_choice_rating_sentiment_no_neutral_task_light.yaml",
      "task_name": "arabic_leaderboard_alghafa_multiple_choice_rating_sentiment_no_neutral_task_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/AlGhafa-Arabic-LLM-Benchmark-Native-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'gold' is undefined",
      "test_time_seconds": 0.56
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_alghafa_light/arabic_leaderboard_alghafa_multiple_choice_rating_sentiment_task_light.yaml",
      "task_name": "arabic_leaderboard_alghafa_multiple_choice_rating_sentiment_task_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/AlGhafa-Arabic-LLM-Benchmark-Native-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'gold' is undefined",
      "test_time_seconds": 0.55
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_alghafa_light/arabic_leaderboard_alghafa_multiple_choice_sentiment_task_light.yaml",
      "task_name": "arabic_leaderboard_alghafa_multiple_choice_sentiment_task_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/AlGhafa-Arabic-LLM-Benchmark-Native-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'gold' is undefined",
      "test_time_seconds": 0.66
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_exams_light/arabic_exams_light.yaml",
      "task_name": "arabic_exams_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_EXAMS-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 1.47
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_abstract_algebra_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_abstract_algebra_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.91
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_anatomy_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_anatomy_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.65
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_astronomy_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_astronomy_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.73
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_business_ethics_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_business_ethics_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.66
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_clinical_knowledge_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_clinical_knowledge_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.78
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_college_biology_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_college_biology_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.7
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_college_chemistry_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_college_chemistry_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.67
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_college_computer_science_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_college_computer_science_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.73
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_college_mathematics_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_college_mathematics_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.68
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_college_medicine_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_college_medicine_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.76
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_college_physics_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_college_physics_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.7
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_computer_security_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_computer_security_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.65
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_conceptual_physics_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_conceptual_physics_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.73
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_econometrics_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_econometrics_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.66
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_electrical_engineering_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_electrical_engineering_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.7
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_elementary_mathematics_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_elementary_mathematics_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.67
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_formal_logic_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_formal_logic_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.65
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_global_facts_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_global_facts_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.83
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_high_school_biology_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_high_school_biology_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.65
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_high_school_chemistry_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_high_school_chemistry_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.8
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_high_school_computer_science_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_high_school_computer_science_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.71
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_high_school_european_history_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_high_school_european_history_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 10.96
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_high_school_geography_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_high_school_geography_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 1.32
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_high_school_government_and_politics_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_high_school_government_and_politics_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.75
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_high_school_macroeconomics_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_high_school_macroeconomics_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.92
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_high_school_mathematics_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_high_school_mathematics_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 3.13
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_high_school_microeconomics_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_high_school_microeconomics_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.63
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_high_school_physics_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_high_school_physics_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.85
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_high_school_psychology_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_high_school_psychology_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.6
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_high_school_statistics_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_high_school_statistics_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.61
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_high_school_us_history_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_high_school_us_history_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 12.91
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_high_school_world_history_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_high_school_world_history_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.68
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_human_aging_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_human_aging_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.69
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_human_sexuality_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_human_sexuality_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.75
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_international_law_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_international_law_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.67
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_jurisprudence_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_jurisprudence_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.71
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_logical_fallacies_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_logical_fallacies_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.81
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_machine_learning_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_machine_learning_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.65
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_management_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_management_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.78
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_marketing_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_marketing_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.75
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_medical_genetics_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_medical_genetics_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.78
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_miscellaneous_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_miscellaneous_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.74
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_moral_disputes_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_moral_disputes_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.71
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_moral_scenarios_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_moral_scenarios_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.76
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_nutrition_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_nutrition_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.8
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_philosophy_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_philosophy_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.92
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_prehistory_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_prehistory_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.65
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_professional_accounting_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_professional_accounting_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.68
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_professional_law_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_professional_law_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.89
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_professional_medicine_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_professional_medicine_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.69
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_professional_psychology_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_professional_psychology_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.72
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_public_relations_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_public_relations_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.71
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_security_studies_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_security_studies_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.65
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_sociology_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_sociology_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.69
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_us_foreign_policy_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_us_foreign_policy_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.67
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_virology_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_virology_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.66
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mmlu_light/arabic_leaderboard_arabic_mmlu_world_religions_light.yaml",
      "task_name": "arabic_leaderboard_arabic_mmlu_world_religions_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/Arabic_MMLU-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.68
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mt_arc_challenge_light/arabic_mt_arc_challenge_light.yaml",
      "task_name": "arabic_mt_arc_challenge_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/AlGhafa-Arabic-LLM-Benchmark-Translated-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'gold' is undefined",
      "test_time_seconds": 0.75
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mt_arc_easy_light/arabic_mt_arc_easy_light.yaml",
      "task_name": "arabic_mt_arc_easy_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/AlGhafa-Arabic-LLM-Benchmark-Translated-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'gold' is undefined",
      "test_time_seconds": 0.62
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mt_boolq_light/arabic_mt_boolq_light.yaml",
      "task_name": "arabic_mt_boolq_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/AlGhafa-Arabic-LLM-Benchmark-Translated-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.64
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mt_copa_light/arabic_mt_copa_light.yaml",
      "task_name": "arabic_mt_copa_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/AlGhafa-Arabic-LLM-Benchmark-Translated-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 2.49
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mt_hellaswag_light/arabic_mt_hellaswag_light.yaml",
      "task_name": "arabic_mt_hellaswag_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/AlGhafa-Arabic-LLM-Benchmark-Translated-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.62
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mt_mmlu_light/arabic_mt_mmlu_light.yaml",
      "task_name": "arabic_mt_mmlu_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/AlGhafa-Arabic-LLM-Benchmark-Translated-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'gold' is undefined",
      "test_time_seconds": 0.65
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mt_openbook_qa_light/arabic_mt_openbook_qa_light.yaml",
      "task_name": "arabic_mt_openbook_qa_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/AlGhafa-Arabic-LLM-Benchmark-Translated-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'gold' is undefined",
      "test_time_seconds": 0.63
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mt_piqa_light/arabic_mt_piqa_light.yaml",
      "task_name": "arabic_mt_piqa_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/AlGhafa-Arabic-LLM-Benchmark-Translated-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'gold' is undefined",
      "test_time_seconds": 0.59
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mt_race_light/arabic_mt_race_light.yaml",
      "task_name": "arabic_mt_race_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/AlGhafa-Arabic-LLM-Benchmark-Translated-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'gold' is undefined",
      "test_time_seconds": 0.67
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mt_sciq_light/arabic_mt_sciq_light.yaml",
      "task_name": "arabic_mt_sciq_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/AlGhafa-Arabic-LLM-Benchmark-Translated-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.91
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_arabic_mt_toxigen_light/arabic_mt_toxigen_light.yaml",
      "task_name": "arabic_mt_toxigen_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/AlGhafa-Arabic-LLM-Benchmark-Translated-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.54
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Algeria_light.yaml",
      "task_name": "arabic_leaderboard_acva_Algeria_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 1.65
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Ancient_Egypt_light.yaml",
      "task_name": "arabic_leaderboard_acva_Ancient_Egypt_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.6
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Arab_Empire_light.yaml",
      "task_name": "arabic_leaderboard_acva_Arab_Empire_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.64
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Arabic_Architecture_light.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Architecture_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.69
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Arabic_Art_light.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Art_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.62
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Arabic_Astronomy_light.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Astronomy_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.62
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Arabic_Calligraphy_light.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Calligraphy_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.66
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Arabic_Ceremony_light.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Ceremony_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.61
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Arabic_Clothing_light.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Clothing_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.62
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Arabic_Culture_light.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Culture_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.69
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Arabic_Food_light.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Food_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.6
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Arabic_Funeral_light.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Funeral_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.61
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Arabic_Geography_light.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Geography_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.59
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Arabic_History_light.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_History_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.76
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Arabic_Language_Origin_light.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Language_Origin_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.64
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Arabic_Literature_light.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Literature_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.6
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Arabic_Math_light.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Math_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.62
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Arabic_Medicine_light.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Medicine_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.59
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Arabic_Music_light.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Music_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.59
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Arabic_Ornament_light.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Ornament_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 2.28
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Arabic_Philosophy_light.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Philosophy_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.63
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Arabic_Physics_and_Chemistry_light.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Physics_and_Chemistry_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.68
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Arabic_Wedding_light.yaml",
      "task_name": "arabic_leaderboard_acva_Arabic_Wedding_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.71
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Bahrain_light.yaml",
      "task_name": "arabic_leaderboard_acva_Bahrain_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 1.2
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Comoros_light.yaml",
      "task_name": "arabic_leaderboard_acva_Comoros_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.63
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Egypt_modern_light.yaml",
      "task_name": "arabic_leaderboard_acva_Egypt_modern_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.7
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_InfluenceFromAncientEgypt_light.yaml",
      "task_name": "arabic_leaderboard_acva_InfluenceFromAncientEgypt_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 2.11
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_InfluenceFromByzantium_light.yaml",
      "task_name": "arabic_leaderboard_acva_InfluenceFromByzantium_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.64
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_InfluenceFromChina_light.yaml",
      "task_name": "arabic_leaderboard_acva_InfluenceFromChina_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.59
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_InfluenceFromGreece_light.yaml",
      "task_name": "arabic_leaderboard_acva_InfluenceFromGreece_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.65
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_InfluenceFromIslam_light.yaml",
      "task_name": "arabic_leaderboard_acva_InfluenceFromIslam_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.66
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_InfluenceFromPersia_light.yaml",
      "task_name": "arabic_leaderboard_acva_InfluenceFromPersia_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.83
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_InfluenceFromRome_light.yaml",
      "task_name": "arabic_leaderboard_acva_InfluenceFromRome_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.66
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Iraq_light.yaml",
      "task_name": "arabic_leaderboard_acva_Iraq_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.65
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Islam_Education_light.yaml",
      "task_name": "arabic_leaderboard_acva_Islam_Education_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.62
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Islam_branches_and_schools_light.yaml",
      "task_name": "arabic_leaderboard_acva_Islam_branches_and_schools_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.67
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Islamic_law_system_light.yaml",
      "task_name": "arabic_leaderboard_acva_Islamic_law_system_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.61
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Jordan_light.yaml",
      "task_name": "arabic_leaderboard_acva_Jordan_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.6
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Kuwait_light.yaml",
      "task_name": "arabic_leaderboard_acva_Kuwait_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.6
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Lebanon_light.yaml",
      "task_name": "arabic_leaderboard_acva_Lebanon_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.68
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Libya_light.yaml",
      "task_name": "arabic_leaderboard_acva_Libya_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.6
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Mauritania_light.yaml",
      "task_name": "arabic_leaderboard_acva_Mauritania_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.61
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Mesopotamia_civilization_light.yaml",
      "task_name": "arabic_leaderboard_acva_Mesopotamia_civilization_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.74
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Morocco_light.yaml",
      "task_name": "arabic_leaderboard_acva_Morocco_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.65
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Oman_light.yaml",
      "task_name": "arabic_leaderboard_acva_Oman_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.61
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Palestine_light.yaml",
      "task_name": "arabic_leaderboard_acva_Palestine_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.73
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Qatar_light.yaml",
      "task_name": "arabic_leaderboard_acva_Qatar_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.66
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Saudi_Arabia_light.yaml",
      "task_name": "arabic_leaderboard_acva_Saudi_Arabia_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.69
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Somalia_light.yaml",
      "task_name": "arabic_leaderboard_acva_Somalia_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.69
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Sudan_light.yaml",
      "task_name": "arabic_leaderboard_acva_Sudan_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.74
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Syria_light.yaml",
      "task_name": "arabic_leaderboard_acva_Syria_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.7
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Tunisia_light.yaml",
      "task_name": "arabic_leaderboard_acva_Tunisia_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.58
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_United_Arab_Emirates_light.yaml",
      "task_name": "arabic_leaderboard_acva_United_Arab_Emirates_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.59
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_Yemen_light.yaml",
      "task_name": "arabic_leaderboard_acva_Yemen_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "OALL/ACVA",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.59
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_communication_light.yaml",
      "task_name": "arabic_leaderboard_acva_communication_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.63
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_computer_and_phone_light.yaml",
      "task_name": "arabic_leaderboard_acva_computer_and_phone_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.65
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_daily_life_light.yaml",
      "task_name": "arabic_leaderboard_acva_daily_life_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.6
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arabic_leaderboard_light/arabic_leaderboard_avca_light/arabic_leaderboard_acva_entertainment_light.yaml",
      "task_name": "arabic_leaderboard_acva_entertainment_light",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "arcee-globe/ACVA-10percent",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.64
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/aradice/boolq/EGY/boolq_egy.yaml",
      "task_name": "AraDiCE_boolq_egy",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "QCRI/AraDiCE-BoolQ",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.4
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/aradice/boolq/ENG/boolq_eng.yaml",
      "task_name": "AraDiCE_boolq_eng",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "QCRI/AraDiCE-BoolQ",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.99
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/aradice/boolq/LEV/boolq_lev.yaml",
      "task_name": "AraDiCE_boolq_lev",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "QCRI/AraDiCE-BoolQ",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.27
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/aradice/boolq/MSA/boolq_msa.yaml",
      "task_name": "AraDiCE_boolq_msa",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "QCRI/AraDiCE-BoolQ",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.12
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/aradice/cultural-benchmark/egypt.yaml",
      "task_name": "AraDiCE_egypt_cultural",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "QCRI/AraDiCE-Culture",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.93
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/aradice/cultural-benchmark/jordan.yaml",
      "task_name": "AraDiCE_jordan_cultural",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "QCRI/AraDiCE-Culture",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.63
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/aradice/cultural-benchmark/lebanon.yaml",
      "task_name": "AraDiCE_lebanon_cultural",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "QCRI/AraDiCE-Culture",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.61
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/aradice/cultural-benchmark/palestine.yaml",
      "task_name": "AraDiCE_palestine_cultural",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "QCRI/AraDiCE-Culture",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.67
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/aradice/cultural-benchmark/qatar.yaml",
      "task_name": "AraDiCE_qatar_cultural",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "QCRI/AraDiCE-Culture",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.61
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/aradice/cultural-benchmark/syria.yaml",
      "task_name": "AraDiCE_syria_cultural",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "QCRI/AraDiCE-Culture",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.65
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/aradice/openbookqa/openbookqa_egy.yaml",
      "task_name": "AraDiCE_openbookqa_egy",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "QCRI/AraDiCE-OpenBookQA",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.06
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/aradice/openbookqa/openbookqa_eng.yaml",
      "task_name": "AraDiCE_openbookqa_eng",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "QCRI/AraDiCE-OpenBookQA",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.82
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/aradice/openbookqa/openbookqa_lev.yaml",
      "task_name": "AraDiCE_openbookqa_lev",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "QCRI/AraDiCE-OpenBookQA",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.87
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/aradice/openbookqa/openbookqa_msa.yaml",
      "task_name": "AraDiCE_openbookqa_msa",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "QCRI/AraDiCE-OpenBookQA",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.97
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/aradice/piqa/piqa_egy.yaml",
      "task_name": "AraDiCE_piqa_egy",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "QCRI/AraDiCE-PIQA",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 12.56
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/aradice/piqa/piqa_eng.yaml",
      "task_name": "AraDiCE_piqa_eng",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "QCRI/AraDiCE-PIQA",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.72
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/aradice/piqa/piqa_lev.yaml",
      "task_name": "AraDiCE_piqa_lev",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "QCRI/AraDiCE-PIQA",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.21
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/aradice/piqa/piqa_msa.yaml",
      "task_name": "AraDiCE_piqa_msa",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "QCRI/AraDiCE-PIQA",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.07
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/aradice/truthfulqa_mcq/truthfulqa_mc1_egy.yaml",
      "task_name": "AraDiCE_truthfulqa_mc1_egy",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "QCRI/AraDiCE-TruthfulQA",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.98
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/aradice/truthfulqa_mcq/truthfulqa_mc1_eng.yaml",
      "task_name": "AraDiCE_truthfulqa_mc1_eng",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "QCRI/AraDiCE-TruthfulQA",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.84
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/aradice/truthfulqa_mcq/truthfulqa_mc1_lev.yaml",
      "task_name": "AraDiCE_truthfulqa_mc1_lev",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "QCRI/AraDiCE-TruthfulQA",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.87
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/aradice/truthfulqa_mcq/truthfulqa_mc1_msa.yaml",
      "task_name": "AraDiCE_truthfulqa_mc1_msa",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "QCRI/AraDiCE-TruthfulQA",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.78
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/aradice/winogrande/winogrande_egy.yaml",
      "task_name": "AraDiCE_winogrande_egy",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "QCRI/AraDiCE-WinoGrande",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.87
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/aradice/winogrande/winogrande_eng.yaml",
      "task_name": "AraDiCE_winogrande_eng",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "QCRI/AraDiCE-WinoGrande",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.77
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/aradice/winogrande/winogrande_lev.yaml",
      "task_name": "AraDiCE_winogrande_lev",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "QCRI/AraDiCE-WinoGrande",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.74
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/aradice/winogrande/winogrande_msa.yaml",
      "task_name": "AraDiCE_winogrande_msa",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "QCRI/AraDiCE-WinoGrande",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.7
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arc/arc_challenge_chat.yaml",
      "task_name": "arc_challenge_chat",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "allenai/ai2_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.78
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arc/arc_easy.yaml",
      "task_name": "arc_easy",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "allenai/ai2_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.55
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arc_mt/arc_challenge_mt_fi.yaml",
      "task_name": "arc_challenge_mt_fi",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "LumiOpen/arc_challenge_mt",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.92
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arc_mt/arc_challenge_mt_is.yaml",
      "task_name": "arc_challenge_mt_is",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "mideind/icelandic-arc-challenge",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.95
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/arithmetic/arithmetic_1dc.yaml",
      "task_name": "arithmetic_1dc",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "EleutherAI/arithmetic",
      "dataset_load_error": "Dataset scripts are no longer supported, but found arithmetic.py",
      "instance_build_error": "",
      "test_time_seconds": 0.48
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/asdiv/asdiv-cot-llama.yaml",
      "task_name": "asdiv_cot_llama",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "EleutherAI/asdiv",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.08
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/asdiv/default.yaml",
      "task_name": "asdiv",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "EleutherAI/asdiv",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.83
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/babi/babi.yaml",
      "task_name": "babi",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Muennighoff/babi",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.36
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/bangla/bangla_mmlu_test.yaml",
      "task_name": "bangla_mmlu",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hishab/titulm-bangla-mmlu",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.98
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/basque_bench/arc_eu_easy.yaml",
      "task_name": "arc_eu_easy",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "HiTZ/ARC-eu",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.74
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/basque_bench/mgsm_cot_native_eu.yaml",
      "task_name": "mgsm_native_cot_eu",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "HiTZ/MGSM-eu",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.75
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/basque_bench/mgsm_direct_eu.yaml",
      "task_name": "mgsm_direct_eu",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "HiTZ/MGSM-eu",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.42
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/basque_bench/paws_eu.yaml",
      "task_name": "paws_eu",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "HiTZ/PAWS-eu",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.72
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/basque_bench/piqa_eu.yaml",
      "task_name": "piqa_eu",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "HiTZ/PIQA-eu",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 3.11
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/basque_bench/wnli_eu.yaml",
      "task_name": "wnli_eu",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "HiTZ/wnli-eu",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.94
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/basque_bench/xcopa_eu.yaml",
      "task_name": "xcopa_eu",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "HiTZ/XCOPA-eu",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.56
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/basqueglue/bec.yaml",
      "task_name": "bec2016eu",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "orai-nlp/basqueGLUE",
      "dataset_load_error": "Dataset scripts are no longer supported, but found basqueGLUE.py",
      "instance_build_error": "",
      "test_time_seconds": 0.77
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/basqueglue/bhtc.yaml",
      "task_name": "bhtc_v2",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "orai-nlp/basqueGLUE",
      "dataset_load_error": "Dataset scripts are no longer supported, but found basqueGLUE.py",
      "instance_build_error": "",
      "test_time_seconds": 0.35
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/basqueglue/coref.yaml",
      "task_name": "epec_koref_bin",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "orai-nlp/basqueGLUE",
      "dataset_load_error": "Dataset scripts are no longer supported, but found basqueGLUE.py",
      "instance_build_error": "",
      "test_time_seconds": 0.34
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/basqueglue/qnli.yaml",
      "task_name": "qnlieu",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "orai-nlp/basqueGLUE",
      "dataset_load_error": "Dataset scripts are no longer supported, but found basqueGLUE.py",
      "instance_build_error": "",
      "test_time_seconds": 0.39
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/basqueglue/vaxx.yaml",
      "task_name": "vaxx_stance",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "orai-nlp/basqueGLUE",
      "dataset_load_error": "Dataset scripts are no longer supported, but found basqueGLUE.py",
      "instance_build_error": "",
      "test_time_seconds": 0.34
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/basqueglue/wic.yaml",
      "task_name": "wiceu",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "orai-nlp/basqueGLUE",
      "dataset_load_error": "Dataset scripts are no longer supported, but found basqueGLUE.py",
      "instance_build_error": "",
      "test_time_seconds": 0.35
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/bbq/bbq_generate.yaml",
      "task_name": "bbq_generate",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "oskarvanderwal/bbq",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 4.46
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/bbq/bbq_multiple_choice.yaml",
      "task_name": "bbq",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "oskarvanderwal/bbq",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.61
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/benchmarks/t0_eval.yaml",
      "task_name": [
        {
          "dataset_path": "super_glue",
          "dataset_name": "wsc.fixed",
          "use_prompt": "promptsource:*",
          "training_split": "train",
          "validation_split": "validation",
          "output_type": "generate_until",
          "metric_list": [
            {
              "metric": "exact_match",
              "aggregation": "mean",
              "higher_is_better": true,
              "ignore_case": true,
              "ignore_punctuation": true
            }
          ]
        },
        {
          "dataset_path": "winogrande",
          "dataset_name": "winogrande_xl",
          "use_prompt": "promptsource:*",
          "training_split": "train",
          "validation_split": "validation",
          "output_type": "generate_until",
          "metric_list": [
            {
              "metric": "exact_match",
              "aggregation": "mean",
              "higher_is_better": true,
              "ignore_case": true,
              "ignore_punctuation": true
            }
          ]
        },
        {
          "dataset_path": "super_glue",
          "dataset_name": "cb",
          "use_prompt": "promptsource:*",
          "training_split": "train",
          "validation_split": "validation",
          "output_type": "generate_until",
          "metric_list": [
            {
              "metric": "exact_match",
              "aggregation": "mean",
              "higher_is_better": true,
              "ignore_case": true,
              "ignore_punctuation": true
            }
          ]
        },
        {
          "dataset_path": "super_glue",
          "dataset_name": "rte",
          "use_prompt": "promptsource:*",
          "training_split": "train",
          "validation_split": "validation",
          "output_type": "generate_until",
          "metric_list": [
            {
              "metric": "exact_match",
              "aggregation": "mean",
              "higher_is_better": true,
              "ignore_case": true,
              "ignore_punctuation": true
            }
          ]
        },
        {
          "task": "anli_r1",
          "dataset_path": "anli",
          "use_prompt": "promptsource:*",
          "training_split": "train_r1",
          "validation_split": "dev_r1",
          "output_type": "generate_until",
          "metric_list": [
            {
              "metric": "exact_match",
              "aggregation": "mean",
              "higher_is_better": true,
              "ignore_case": true,
              "ignore_punctuation": true
            }
          ]
        },
        {
          "task": "anli_r2",
          "dataset_path": "anli",
          "use_prompt": "promptsource:*",
          "training_split": "train_r2",
          "validation_split": "dev_r2",
          "output_type": "generate_until",
          "metric_list": [
            {
              "metric": "exact_match",
              "aggregation": "mean",
              "higher_is_better": true,
              "ignore_case": true,
              "ignore_punctuation": true
            }
          ]
        },
        {
          "task": "anli_r3",
          "dataset_path": "anli",
          "use_prompt": "promptsource:*",
          "training_split": "train_r3",
          "validation_split": "dev_r3",
          "output_type": "generate_until",
          "metric_list": [
            {
              "metric": "exact_match",
              "aggregation": "mean",
              "higher_is_better": true,
              "ignore_case": true,
              "ignore_punctuation": true
            }
          ]
        },
        {
          "dataset_path": "super_glue",
          "dataset_name": "copa",
          "use_prompt": "promptsource:*",
          "training_split": "train",
          "validation_split": "validation",
          "output_type": "generate_until",
          "metric_list": [
            {
              "metric": "exact_match",
              "aggregation": "mean",
              "higher_is_better": true,
              "ignore_case": true,
              "ignore_punctuation": true
            }
          ]
        },
        {
          "dataset_path": "hellaswag",
          "use_prompt": "promptsource:*",
          "training_split": "train",
          "validation_split": "validation",
          "output_type": "generate_until",
          "metric_list": [
            {
              "metric": "exact_match",
              "aggregation": "mean",
              "higher_is_better": true,
              "ignore_case": true,
              "ignore_punctuation": true
            }
          ]
        },
        {
          "dataset_path": "super_glue",
          "dataset_name": "wic",
          "use_prompt": "promptsource:*",
          "training_split": "train",
          "validation_split": "validation",
          "output_type": "generate_until",
          "metric_list": [
            {
              "metric": "exact_match",
              "aggregation": "mean",
              "higher_is_better": true,
              "ignore_case": true,
              "ignore_punctuation": true
            }
          ]
        }
      ],
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "",
      "dataset_load_error": "list index out of range",
      "instance_build_error": "",
      "test_time_seconds": 0.01
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/c4/c4.yaml",
      "task_name": "c4",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "allenai/c4",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 4.75
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/careqa/careqa_en.yaml",
      "task_name": "careqa_en",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "HPAI-BSC/CareQA",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.42
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/careqa/careqa_open.yaml",
      "task_name": "careqa_open",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "HPAI-BSC/CareQA",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.89
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/catalan_bench/catalanqa.yaml",
      "task_name": "catalanqa",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "projecte-aina/catalanqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.14
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/catalan_bench/catcola.yaml",
      "task_name": "catcola",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "nbel/CatCoLA",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.9
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/catalan_bench/cocoteros_va.yaml",
      "task_name": "cocoteros_va",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "gplsi/cocoteros_va",
      "dataset_load_error": "Dataset 'gplsi/cocoteros_va' is a gated dataset on the Hub. You must be authenticated to access it.",
      "instance_build_error": "",
      "test_time_seconds": 0.62
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/catalan_bench/copa_ca.yaml",
      "task_name": "copa_ca",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "projecte-aina/COPA-ca",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.19
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/catalan_bench/coqcat.yaml",
      "task_name": "coqcat",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "projecte-aina/CoQCat",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.23
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/catalan_bench/mgsm_direct_ca.yaml",
      "task_name": "mgsm_direct_ca",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "projecte-aina/mgsm_ca",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.16
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/catalan_bench/openbookqa_ca.yaml",
      "task_name": "openbookqa_ca",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "projecte-aina/openbookqa_ca",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.32
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/catalan_bench/parafraseja.yaml",
      "task_name": "parafraseja",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "projecte-aina/Parafraseja",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.33
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/catalan_bench/paws_ca.yaml",
      "task_name": "paws_ca",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "projecte-aina/PAWS-ca",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.64
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/catalan_bench/piqa_ca.yaml",
      "task_name": "piqa_ca",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "projecte-aina/piqa_ca",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.84
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/catalan_bench/siqa_ca.yaml",
      "task_name": "siqa_ca",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "projecte-aina/siqa_ca",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.86
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/catalan_bench/teca.yaml",
      "task_name": "teca",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "projecte-aina/teca",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.93
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/catalan_bench/wnli_ca.yaml",
      "task_name": "wnli_ca",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "projecte-aina/wnli-ca",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.09
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/catalan_bench/xnli_ca.yaml",
      "task_name": "xnli_ca",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "projecte-aina/xnli-ca",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.32
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/catalan_bench/xnli_va.yaml",
      "task_name": "xnli_va",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "gplsi/xnli_va",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 6.78
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/catalan_bench/xquad_ca.yaml",
      "task_name": "xquad_ca",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "projecte-aina/xquad-ca",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.83
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/catalan_bench/xstorycloze_ca.yaml",
      "task_name": "xstorycloze_ca",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "projecte-aina/xstorycloze_ca",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.77
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/chartqa/chartqa.yaml",
      "task_name": "chartqa",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "HuggingFaceM4/ChartQA",
      "dataset_load_error": "To support decoding images, please install 'Pillow'.",
      "instance_build_error": "",
      "test_time_seconds": 10.44
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/cnn_dailymail/cnn_dailymail.yaml",
      "task_name": "cnn_dailymail_abisee",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "abisee/cnn_dailymail",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.42
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/code_x_glue/code-text/go.yaml",
      "task_name": "code2text_go",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "CM/codexglue_code2text_go",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.11
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/code_x_glue/code-text/java.yaml",
      "task_name": "code2text_java",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "CM/codexglue_code2text_java",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.23
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/code_x_glue/code-text/javascript.yaml",
      "task_name": "code2text_javascript",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "CM/codexglue_code2text_javascript",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.74
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/code_x_glue/code-text/php.yaml",
      "task_name": "code2text_php",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "CM/codexglue_code2text_php",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 3.78
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/code_x_glue/code-text/python.yaml",
      "task_name": "code2text_python",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "CM/codexglue_code2text_python",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.85
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/code_x_glue/code-text/ruby.yaml",
      "task_name": "code2text_ruby",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "CM/codexglue_code2text_ruby",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.97
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/common_voice/common_voice_en.yaml",
      "task_name": "common_voice_en",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "fixie-ai/endpointing-audio",
      "dataset_load_error": "Dataset 'fixie-ai/endpointing-audio' doesn't exist on the Hub or cannot be accessed.",
      "instance_build_error": "",
      "test_time_seconds": 0.25
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/commonsense_qa/default.yaml",
      "task_name": "commonsense_qa",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "tau/commonsense_qa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.69
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/copal_id/standard.yaml",
      "task_name": "copal_id_standard",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "haryoaw/COPAL",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.92
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/coqa/default.yaml",
      "task_name": "coqa",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "EleutherAI/coqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.3
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/crows_pairs/crows_pairs_english.yaml",
      "task_name": "crows_pairs_english",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "jannalu/crows_pairs_multilingual",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.39
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/darijahellaswag/darijahellaswag.yaml",
      "task_name": "darijahellaswag",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "MBZUAI-Paris/DarijaHellaSwag",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 1.6
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/discrim_eval/discrim_eval_explicit.yaml",
      "task_name": "discrim_eval_explicit",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Anthropic/discrim-eval",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 3.79
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/discrim_eval/discrim_eval_implicit.yaml",
      "task_name": "discrim_eval_implicit",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Anthropic/discrim-eval",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.95
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/drop/default.yaml",
      "task_name": "drop",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "EleutherAI/drop",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.77
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/e2lmc/mmlu_early_training/mmlu_early_training.yaml",
      "task_name": "mmlu_early_training",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "giovanivaldrighi/mmlu",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.29
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/egyhellaswag/egyhellaswag.yaml",
      "task_name": "egyhellaswag",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "UBC-NLP/EgyHellaSwag",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 1.94
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/eq_bench/default.yaml",
      "task_name": "eq_bench",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "pbevan11/EQ-Bench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.26
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/eq_bench/multilingual/eqbench_ca.yaml",
      "task_name": "eqbench_ca",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "BSC-LT/EQ-bench_ca",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.15
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/eq_bench/multilingual/eqbench_es.yaml",
      "task_name": "eqbench_es",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "BSC-LT/EQ-bench_es",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.05
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/eus_proficiency/eus_proficiency.yaml",
      "task_name": "eus_proficiency",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "HiTZ/EusProficiency",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.31
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/eus_reading/eus_reading.yaml",
      "task_name": "eus_reading",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "HiTZ/EusReading",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.91
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/eus_trivia/eus_trivia.yaml",
      "task_name": "eus_trivia",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "HiTZ/EusTrivia",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.1
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/fld/fld_default.yaml",
      "task_name": "fld_default",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hitachi-nlp/FLD.v2",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.06
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/fld/fld_logical_formula_default.yaml",
      "task_name": "fld_logical_formula_default",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hitachi-nlp/FLD.v2",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.58
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/french_bench/french_bench_arc_challenge.yaml",
      "task_name": "french_bench_arc_challenge",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "manu/french_bench_arc_challenge",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.09
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/french_bench/french_bench_boolqa.yaml",
      "task_name": "french_bench_boolqa",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "manu/french_boolq",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 13.37
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/french_bench/french_bench_fquadv2.yaml",
      "task_name": "french_bench_fquadv2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "manu/fquad2_test",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.96
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/french_bench/french_bench_fquadv2_bool.yaml",
      "task_name": "french_bench_fquadv2_bool",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "manu/fquad2_test",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.17
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/french_bench/french_bench_fquadv2_genq.yaml",
      "task_name": "french_bench_fquadv2_genq",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "manu/fquad2_test",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.47
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/french_bench/french_bench_fquadv2_hasAns.yaml",
      "task_name": "french_bench_fquadv2_hasAns",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "manu/fquad2_test",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.49
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/french_bench/french_bench_grammar.yaml",
      "task_name": "french_bench_grammar",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "manu/french-bench-grammar-vocab-reading",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 8.27
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/french_bench/french_bench_hellaswag.yaml",
      "task_name": "french_bench_hellaswag",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "manu/french_bench_hellaswag",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 2.25
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/french_bench/french_bench_multifquad.yaml",
      "task_name": "french_bench_multifquad",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "manu/multifquad_test",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.02
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/french_bench/french_bench_opus_perplexity.yaml",
      "task_name": "french_bench_opus_perplexity",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "manu/opus100-en-fr",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.35
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/french_bench/french_bench_orangesum_abstract.yaml",
      "task_name": "french_bench_orangesum_abstract",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "orange_sum",
      "dataset_load_error": "Dataset scripts are no longer supported, but found orange_sum.py",
      "instance_build_error": "",
      "test_time_seconds": 0.88
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/french_bench/french_bench_orangesum_title.yaml",
      "task_name": "french_bench_orangesum_title",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "orange_sum",
      "dataset_load_error": "Dataset scripts are no longer supported, but found orange_sum.py",
      "instance_build_error": "",
      "test_time_seconds": 0.59
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/french_bench/french_bench_reading_comp.yaml",
      "task_name": "french_bench_reading_comp",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "manu/french-bench-grammar-vocab-reading",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.73
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/french_bench/french_bench_topic_based_nli.yaml",
      "task_name": "french_bench_topic_based_nli",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "manu/topic_based_nli_test",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.38
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/french_bench/french_bench_trivia.yaml",
      "task_name": "french_bench_trivia",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "manu/french-trivia",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.09
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/french_bench/french_bench_vocab.yaml",
      "task_name": "french_bench_vocab",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "manu/french-bench-grammar-vocab-reading",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.72
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/french_bench/french_bench_wikitext_fr.yaml",
      "task_name": "french_bench_wikitext_fr",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "asi/wikitext_fr",
      "dataset_load_error": "Dataset scripts are no longer supported, but found wikitext_fr.py",
      "instance_build_error": "",
      "test_time_seconds": 0.75
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/french_bench/french_bench_xnli.yaml",
      "task_name": "french_bench_xnli",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "xnli",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.16
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/galician_bench/belebele_glg_Latn.yaml",
      "task_name": "belebele_glg_Latn",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "proxectonos/belebele_gl",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.85
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/galician_bench/galcola.yaml",
      "task_name": "galcola",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "proxectonos/galcola",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.01
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/galician_bench/mgsm_direct_gl.yaml",
      "task_name": "mgsm_direct_gl",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "proxectonos/mgsm_gl",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.91
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/galician_bench/openbookqa_gl.yaml",
      "task_name": "openbookqa_gl",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "proxectonos/openbookqa_gl",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 11.62
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/galician_bench/parafrases_gl.yaml",
      "task_name": "parafrases_gl",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "proxectonos/parafrases_gl",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.95
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/galician_bench/paws_gl.yaml",
      "task_name": "paws_gl",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "proxectonos/PAWS-gl",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.02
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/galician_bench/summarization_gl.yaml",
      "task_name": "summarization_gl",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "proxectonos/summarization_gl",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.99
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/galician_bench/truthfulqa_gl_gen.yaml",
      "task_name": "truthfulqa_gl_gen",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "proxectonos/truthfulqa_gl",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.88
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/galician_bench/truthfulqa_gl_mc1.yaml",
      "task_name": "truthfulqa_gl_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "proxectonos/truthfulqa_gl",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.15
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/galician_bench/xnli_gl.yaml",
      "task_name": "xnli_gl",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "proxectonos/xnli_gl",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.13
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/galician_bench/xstorycloze_gl.yaml",
      "task_name": "xstorycloze_gl",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "proxectonos/xstorycloze_gl",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.16
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/glianorex/glianorex.yaml",
      "task_name": "glianorex",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "maximegmd/glianorex",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.58
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/glianorex/glianorex_en.yaml",
      "task_name": "glianorex_en",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "maximegmd/glianorex",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.58
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/glianorex/glianorex_fr.yaml",
      "task_name": "glianorex_fr",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "maximegmd/glianorex",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.5
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/glue/cola/default.yaml",
      "task_name": "cola",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "nyu-mll/glue",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.66
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/glue/mnli/default.yaml",
      "task_name": "mnli",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "nyu-mll/glue",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.57
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/glue/mrpc/default.yaml",
      "task_name": "mrpc",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "nyu-mll/glue",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.58
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/glue/qnli/default.yaml",
      "task_name": "qnli",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "nyu-mll/glue",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.57
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/glue/qqp/default.yaml",
      "task_name": "qqp",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "nyu-mll/glue",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.78
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/glue/rte/default.yaml",
      "task_name": "rte",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "nyu-mll/glue",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.15
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/glue/sst2/default.yaml",
      "task_name": "sst2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "nyu-mll/glue",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 11.38
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/glue/wnli/default.yaml",
      "task_name": "wnli",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "nyu-mll/glue",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.49
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/groundcocoa/groundcocoa.yaml",
      "task_name": "groundcocoa",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "harsh147/GroundCocoa",
      "dataset_load_error": "",
      "instance_build_error": "'criteria' is undefined",
      "test_time_seconds": 1.18
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/gsm8k/gsm8k-cot-llama.yaml",
      "task_name": "gsm8k_cot_llama",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "gsm8k",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.18
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/gsm8k/gsm8k-cot-zeroshot.yaml",
      "task_name": "gsm8k_cot_zeroshot",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "gsm8k",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.83
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/gsm8k/gsm8k-cot.yaml",
      "task_name": "gsm8k_cot",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "gsm8k",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.78
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/gsm8k/gsm8k.yaml",
      "task_name": "gsm8k",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "gsm8k",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.83
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/gsm8k_platinum/gsm8k-platinum-cot-llama.yaml",
      "task_name": "gsm8k_platinum_cot_llama",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "madrylab/gsm8k-platinum",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.09
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/gsm8k_platinum/gsm8k-platinum-cot-zeroshot.yaml",
      "task_name": "gsm8k_platinum_cot_zeroshot",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "madrylab/gsm8k-platinum",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.5
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/gsm8k_platinum/gsm8k-platinum-cot.yaml",
      "task_name": "gsm8k_platinum_cot",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "madrylab/gsm8k-platinum",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.48
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/gsm8k_platinum/gsm8k-platinum.yaml",
      "task_name": "gsm8k_platinum",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "madrylab/gsm8k-platinum",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.47
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/gsm_plus/gsm_plus.yaml",
      "task_name": "gsm_plus",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "qintongli/GSM-Plus",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.25
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/gsm_plus/gsm_plus_mini.yaml",
      "task_name": "gsm_plus_mini",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "qintongli/GSM-Plus",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.5
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/headqa/headqa_en.yaml",
      "task_name": "headqa_en",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "EleutherAI/headqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.74
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/hellaswag/hellaswag.yaml",
      "task_name": "hellaswag",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "Rowan/hellaswag",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 1.13
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/hendrycks_ethics/commonsense.yaml",
      "task_name": "ethics_cm",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "EleutherAI/hendrycks_ethics",
      "dataset_load_error": "Dataset scripts are no longer supported, but found hendrycks_ethics.py",
      "instance_build_error": "",
      "test_time_seconds": 0.72
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/hendrycks_math/hendrycks_math500.yaml",
      "task_name": "hendrycks_math500",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "HuggingFaceH4/MATH-500",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.11
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/hendrycks_math/hendrycks_math_algebra.yaml",
      "task_name": "hendrycks_math_algebra",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "EleutherAI/hendrycks_math",
      "dataset_load_error": "",
      "instance_build_error": "'answer' is undefined",
      "test_time_seconds": 0.7
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/histoires_morales/histoires_morales.yaml",
      "task_name": "histoires_morales",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "LabHC/histoires_morales",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 2.01
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/humaneval/humaneval.yaml",
      "task_name": "humaneval",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "openai/openai_humaneval",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.65
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/humaneval/humaneval_plus.yaml",
      "task_name": "humaneval_plus",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "evalplus/humanevalplus",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.08
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/humaneval_infilling/multi_line_infilling.yaml",
      "task_name": "humaneval_multi_line_infilling",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "loubnabnl/humaneval_infilling",
      "dataset_load_error": "Dataset scripts are no longer supported, but found humaneval_infilling.py",
      "instance_build_error": "",
      "test_time_seconds": 0.81
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/icelandic_winogrande/default.yaml",
      "task_name": "icelandic_winogrande",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "mideind/icelandic-winogrande",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.14
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/ifeval/ifeval.yaml",
      "task_name": "ifeval",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "google/IFEval",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.44
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/inverse_scaling/inverse_scaling_hindsight_neglect.yaml",
      "task_name": "inverse_scaling_hindsight_neglect_10shot",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "inverse-scaling/hindsight-neglect-10shot",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.34
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/inverse_scaling/inverse_scaling_into_the_unknown.yaml",
      "task_name": "inverse_scaling_into_the_unknown",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Albertmade/into-the-unknown",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.53
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/inverse_scaling/inverse_scaling_memo_trap.yaml",
      "task_name": "inverse_scaling_memo_trap",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Albertmade/memo-trap",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.94
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/inverse_scaling/inverse_scaling_modus_tollens.yaml",
      "task_name": "inverse_scaling_modus_tollens",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Albertmade/modus-tollens",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 12.36
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/inverse_scaling/inverse_scaling_neqa.yaml",
      "task_name": "inverse_scaling_neqa",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "inverse-scaling/NeQA",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.15
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/inverse_scaling/inverse_scaling_pattern_matching_suppression.yaml",
      "task_name": "inverse_scaling_pattern_matching_suppression",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Albertmade/pattern-matching-suppression",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.12
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/inverse_scaling/inverse_scaling_quote_repetition.yaml",
      "task_name": "inverse_scaling_quote_repetition",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "inverse-scaling/quote-repetition",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.63
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/inverse_scaling/inverse_scaling_redefine_math.yaml",
      "task_name": "inverse_scaling_redefine_math",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "inverse-scaling/redefine-math",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.75
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/inverse_scaling/inverse_scaling_repetitive_algebra.yaml",
      "task_name": "inverse_scaling_repetitive_algebra",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Albertmade/repetitive-algebra",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.02
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/inverse_scaling/inverse_scaling_sig_figs.yaml",
      "task_name": "inverse_scaling_sig_figs",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Albertmade/sig-figs",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.46
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/inverse_scaling/inverse_scaling_winobias_antistereotype.yaml",
      "task_name": "inverse_scaling_winobias_antistereotype",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "mathemakitten/winobias_antistereotype_test_v5",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.94
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/japanese_leaderboard/ja_leaderboard_jaqket_v2.yaml",
      "task_name": "ja_leaderboard_jaqket_v2",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "kumapo/JAQKET",
      "dataset_load_error": "Dataset scripts are no longer supported, but found JAQKET.py",
      "instance_build_error": "",
      "test_time_seconds": 0.65
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/japanese_leaderboard/ja_leaderboard_jcommonsenseqa.yaml",
      "task_name": "ja_leaderboard_jcommonsenseqa",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "Rakuten/JGLUE",
      "dataset_load_error": "Dataset scripts are no longer supported, but found JGLUE.py",
      "instance_build_error": "",
      "test_time_seconds": 1.09
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/japanese_leaderboard/ja_leaderboard_jnli.yaml",
      "task_name": "ja_leaderboard_jnli",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "Rakuten/JGLUE",
      "dataset_load_error": "Dataset scripts are no longer supported, but found JGLUE.py",
      "instance_build_error": "",
      "test_time_seconds": 0.42
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/japanese_leaderboard/ja_leaderboard_jsquad.yaml",
      "task_name": "ja_leaderboard_jsquad",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "Rakuten/JGLUE",
      "dataset_load_error": "Dataset scripts are no longer supported, but found JGLUE.py",
      "instance_build_error": "",
      "test_time_seconds": 0.45
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/japanese_leaderboard/ja_leaderboard_marc_ja.yaml",
      "task_name": "ja_leaderboard_marc_ja",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "Rakuten/JGLUE",
      "dataset_load_error": "Dataset scripts are no longer supported, but found JGLUE.py",
      "instance_build_error": "",
      "test_time_seconds": 0.5
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/japanese_leaderboard/ja_leaderboard_mgsm.yaml",
      "task_name": "ja_leaderboard_mgsm",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "juletxara/mgsm",
      "dataset_load_error": "Dataset scripts are no longer supported, but found mgsm.py",
      "instance_build_error": "",
      "test_time_seconds": 0.64
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/japanese_leaderboard/ja_leaderboard_xlsum.yaml",
      "task_name": "ja_leaderboard_xlsum",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "mkshing/xlsum_ja",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 3.47
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/japanese_leaderboard/ja_leaderboard_xwinograd.yaml",
      "task_name": "ja_leaderboard_xwinograd",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "polm-stability/xwinograd-ja",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 4.4
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/jsonschema_bench/jsonschema_bench_easy.yaml",
      "task_name": "jsonschema_bench_easy",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "epfl-dlab/JSONSchemaBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.14
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/kobest/kobest_boolq.yaml",
      "task_name": "kobest_boolq",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "skt/kobest_v1",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 3.43
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/kobest/kobest_copa.yaml",
      "task_name": "kobest_copa",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "skt/kobest_v1",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.67
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/kobest/kobest_hellaswag.yaml",
      "task_name": "kobest_hellaswag",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "skt/kobest_v1",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 0.82
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/kobest/kobest_sentineg.yaml",
      "task_name": "kobest_sentineg",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "skt/kobest_v1",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.79
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/kobest/kobest_wic.yaml",
      "task_name": "kobest_wic",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "skt/kobest_v1",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.84
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/lambada/lambada_openai.yaml",
      "task_name": "lambada_openai",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "EleutherAI/lambada_openai",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.08
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/lambada/lambada_standard.yaml",
      "task_name": "lambada_standard",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "lambada",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.38
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/lambada_cloze/lambada_openai_cloze.yaml",
      "task_name": "lambada_openai_cloze_yaml",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "EleutherAI/lambada_openai",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.97
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/lambada_cloze/lambada_standard_cloze.yaml",
      "task_name": "lambada_standard_cloze_yaml",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "lambada",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.86
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/lambada_multilingual/lambada_mt_en.yaml",
      "task_name": "lambada_openai_mt_en",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "EleutherAI/lambada_openai",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.75
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/lambada_multilingual_stablelm/lambada_mt_stablelm_en.yaml",
      "task_name": "lambada_openai_mt_stablelm_en",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "EleutherAI/lambada_multilingual_stablelm",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.6
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/leaderboard/ifeval/ifeval.yaml",
      "task_name": "leaderboard_ifeval",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "wis-k/instruction-following-eval",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.63
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/leaderboard/mmlu_pro/mmlu_pro.yaml",
      "task_name": "leaderboard_mmlu_pro",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "TIGER-Lab/MMLU-Pro",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.77
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/lingoly/lingoly_context.yaml",
      "task_name": "lingoly_context",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "ambean/lingOly",
      "dataset_load_error": "Dataset 'ambean/lingOly' is a gated dataset on the Hub. You must be authenticated to access it.",
      "instance_build_error": "",
      "test_time_seconds": 0.74
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/lingoly/lingoly_nocontext.yaml",
      "task_name": "lingoly_nocontext",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "ambean/lingOly",
      "dataset_load_error": "Dataset 'ambean/lingOly' is a gated dataset on the Hub. You must be authenticated to access it.",
      "instance_build_error": "",
      "test_time_seconds": 0.5
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/llama3/instruct/arc_challenge/arc_challenge_llama.yaml",
      "task_name": "arc_challenge_llama",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "allenai/ai2_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.53
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/logiqa/logiqa.yaml",
      "task_name": "logiqa",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "EleutherAI/logiqa",
      "dataset_load_error": "Dataset scripts are no longer supported, but found logiqa.py",
      "instance_build_error": "",
      "test_time_seconds": 0.61
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/logiqa2/logieval.yaml",
      "task_name": "logieval",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "baber/logiqa2",
      "dataset_load_error": "Dataset scripts are no longer supported, but found logiqa2.py",
      "instance_build_error": "",
      "test_time_seconds": 0.61
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/logiqa2/logiqa2.yaml",
      "task_name": "logiqa2",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "baber/logiqa2",
      "dataset_load_error": "Dataset scripts are no longer supported, but found logiqa2.py",
      "instance_build_error": "",
      "test_time_seconds": 0.41
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/2wikimqa.yaml",
      "task_name": "longbench_2wikimqa",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.62
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/2wikimqa_e.yaml",
      "task_name": "longbench_2wikimqa_e",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.45
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/dureader.yaml",
      "task_name": "longbench_dureader",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.3
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/gov_report.yaml",
      "task_name": "longbench_gov_report",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.8
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/gov_report_e.yaml",
      "task_name": "longbench_gov_report_e",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.28
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/hotpotqa.yaml",
      "task_name": "longbench_hotpotqa",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.78
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/hotpotqa_e.yaml",
      "task_name": "longbench_hotpotqa_e",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.34
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/lcc.yaml",
      "task_name": "longbench_lcc",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.76
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/lcc_e.yaml",
      "task_name": "longbench_lcc_e",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.39
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/lsht.yaml",
      "task_name": "longbench_lsht",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.03
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/multi_news.yaml",
      "task_name": "longbench_multi_news",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.68
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/multi_news_e.yaml",
      "task_name": "longbench_multi_news_e",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.01
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/multifieldqa_en.yaml",
      "task_name": "longbench_multifieldqa_en",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.76
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/multifieldqa_en_e.yaml",
      "task_name": "longbench_multifieldqa_en_e",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.11
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/multifieldqa_zh.yaml",
      "task_name": "longbench_multifieldqa_zh",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.71
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/musique.yaml",
      "task_name": "longbench_musique",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.02
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/narrativeqa.yaml",
      "task_name": "longbench_narrativeqa",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.01
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/passage_count.yaml",
      "task_name": "longbench_passage_count",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.04
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/passage_count_e.yaml",
      "task_name": "longbench_passage_count_e",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.07
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/passage_retrieval_en.yaml",
      "task_name": "longbench_passage_retrieval_en",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.82
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/passage_retrieval_en_e.yaml",
      "task_name": "longbench_passage_retrieval_en_e",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.07
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/passage_retrieval_zh.yaml",
      "task_name": "longbench_passage_retrieval_zh",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.74
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/qasper.yaml",
      "task_name": "longbench_qasper",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.35
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/qasper_e.yaml",
      "task_name": "longbench_qasper_e",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.56
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/qmsum.yaml",
      "task_name": "longbench_qmsum",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.72
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/repobench-p.yaml",
      "task_name": "longbench_repobench-p",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.77
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/repobench-p_e.yaml",
      "task_name": "longbench_repobench-p_e",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.08
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/samsum.yaml",
      "task_name": "longbench_samsum",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.79
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/samsum_e.yaml",
      "task_name": "longbench_samsum_e",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.81
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/trec.yaml",
      "task_name": "longbench_trec",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.76
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/trec_e.yaml",
      "task_name": "longbench_trec_e",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.11
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/triviaqa.yaml",
      "task_name": "longbench_triviaqa",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.8
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/triviaqa_e.yaml",
      "task_name": "longbench_triviaqa_e",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.16
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/longbench/vcsum.yaml",
      "task_name": "longbench_vcsum",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Xnhyacinth/LongBench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.71
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/mastermind/mastermind_24_easy.yaml",
      "task_name": "mastermind_24_easy",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "flair/mastermind_24_mcq_random",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.52
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/mastermind/mastermind_24_hard.yaml",
      "task_name": "mastermind_24_hard",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "flair/mastermind_24_mcq_close",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.37
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/mastermind/mastermind_35_easy.yaml",
      "task_name": "mastermind_35_easy",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "flair/mastermind_35_mcq_random",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.78
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/mastermind/mastermind_35_hard.yaml",
      "task_name": "mastermind_35_hard",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "flair/mastermind_35_mcq_close",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 9.59
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/mastermind/mastermind_46_easy.yaml",
      "task_name": "mastermind_46_easy",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "flair/mastermind_46_mcq_random",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.82
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/mastermind/mastermind_46_hard.yaml",
      "task_name": "mastermind_46_hard",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "flair/mastermind_46_mcq_close",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.41
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/mathqa/mathqa.yaml",
      "task_name": "mathqa",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "math_qa",
      "dataset_load_error": "Dataset scripts are no longer supported, but found math_qa.py",
      "instance_build_error": "",
      "test_time_seconds": 0.91
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/mbpp/mbpp.yaml",
      "task_name": "mbpp",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "google-research-datasets/mbpp",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.68
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/mbpp/mbpp_instruct.yaml",
      "task_name": "mbpp_instruct",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "google-research-datasets/mbpp",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.49
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/mbpp/mbpp_plus.yaml",
      "task_name": "mbpp_plus",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "evalplus/mbppplus",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.9
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/mbpp/mbpp_plus_instruct.yaml",
      "task_name": "mbpp_plus_instruct",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "evalplus/mbppplus",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.6
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/mc_taco/default.yaml",
      "task_name": "mc_taco",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "mc_taco",
      "dataset_load_error": "Dataset scripts are no longer supported, but found mc_taco.py",
      "instance_build_error": "",
      "test_time_seconds": 1.01
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/med_prescriptions/med_prescriptions_easy.yaml",
      "task_name": "med_prescriptions_easy",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "devlocalhost/prescription-full",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 15.76
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/med_text_classification/med_text_classification_easy.yaml",
      "task_name": "med_text_classification_easy",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "csv",
      "dataset_load_error": "An error occurred while generating the dataset",
      "instance_build_error": "",
      "test_time_seconds": 817.74
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/meddialog/meddialog_qsumm.yaml",
      "task_name": "meddialog_qsumm",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "lighteval/med_dialog",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 7.17
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/meddialog/meddialog_raw_dialogues.yaml",
      "task_name": "meddialog_raw_dialogues",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "bigbio/meddialog",
      "dataset_load_error": "Dataset scripts are no longer supported, but found meddialog.py",
      "instance_build_error": "",
      "test_time_seconds": 0.89
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/mediqa_qa2019/mediqa_qa2019.yaml",
      "task_name": "mediqa_qa2019",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "bigbio/mediqa_qa",
      "dataset_load_error": "Dataset scripts are no longer supported, but found mediqa_qa.py",
      "instance_build_error": "",
      "test_time_seconds": 0.96
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/mediqa_qa2019/mediqa_qa2019_perplexity.yaml",
      "task_name": "mediqa_qa2019_perplexity",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "bigbio/mediqa_qa",
      "dataset_load_error": "Dataset scripts are no longer supported, but found mediqa_qa.py",
      "instance_build_error": "",
      "test_time_seconds": 0.45
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/medmcqa/medmcqa.yaml",
      "task_name": "medmcqa",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "medmcqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 5.08
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/medqa/medqa.yaml",
      "task_name": "medqa_4options",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "GBaker/MedQA-USMLE-4-options-hf",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.98
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/medtext/medtext.yaml",
      "task_name": "medtext",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "BI55/MedText",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.28
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/mela/mela_en.yaml",
      "task_name": "mela_en",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Geralt-Targaryen/MELA",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.39
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/meqsum/meqsum.yaml",
      "task_name": "meqsum",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "bigbio/meqsum",
      "dataset_load_error": "Dataset scripts are no longer supported, but found meqsum.py",
      "instance_build_error": "",
      "test_time_seconds": 0.81
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/metabench/metabench_arc.yaml",
      "task_name": "metabench_arc",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "HCAI/metabench",
      "dataset_load_error": "",
      "instance_build_error": "'twentyfive_shot_preprompt' is undefined",
      "test_time_seconds": 4.78
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/metabench/metabench_gsm8k.yaml",
      "task_name": "metabench_gsm8k",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "HCAI/metabench",
      "dataset_load_error": "",
      "instance_build_error": "'five_shot_preprompt' is undefined",
      "test_time_seconds": 4.37
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/metabench/metabench_hellaswag.yaml",
      "task_name": "metabench_hellaswag",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "HCAI/metabench",
      "dataset_load_error": "",
      "instance_build_error": "'ten_shot_preprompt' is undefined",
      "test_time_seconds": 2.41
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/metabench/metabench_mmlu.yaml",
      "task_name": "metabench_mmlu",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "HCAI/metabench",
      "dataset_load_error": "",
      "instance_build_error": "'five_shot_preprompt' is undefined",
      "test_time_seconds": 2.41
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/metabench/metabench_truthfulqa.yaml",
      "task_name": "metabench_truthfulqa",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "HCAI/metabench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.49
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/metabench/metabench_winogrande.yaml",
      "task_name": "metabench_winogrande",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "HCAI/metabench",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.26
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/mimic_repsum/mimic_repsum.yaml",
      "task_name": "mimic_repsum",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "dmacres/mimiciii-hospitalcourse-meta",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 13.88
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/minerva_math/minerva_math500.yaml",
      "task_name": "minerva_math500",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "HuggingFaceH4/MATH-500",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.0
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/minerva_math/minerva_math_algebra.yaml",
      "task_name": "minerva_math_algebra",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "EleutherAI/hendrycks_math",
      "dataset_load_error": "",
      "instance_build_error": "'answer' is undefined",
      "test_time_seconds": 0.63
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/mmlu-redux-spanish/mmlu-redux-2.0-spanish.yaml",
      "task_name": "mmlu_redux_spanish",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "amias-mx/mmlu-redux-2.0-spanish",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 5.89
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/model_written_evals/sycophancy/sycophancy_on_nlp_survey.yaml",
      "task_name": "sycophancy_on_nlp_survey",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "EleutherAI/sycophancy",
      "dataset_load_error": "Dataset scripts are no longer supported, but found sycophancy.py",
      "instance_build_error": "",
      "test_time_seconds": 0.86
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/model_written_evals/sycophancy/sycophancy_on_philpapers2020.yaml",
      "task_name": "sycophancy_on_philpapers2020",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "EleutherAI/sycophancy",
      "dataset_load_error": "Dataset scripts are no longer supported, but found sycophancy.py",
      "instance_build_error": "",
      "test_time_seconds": 4.08
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/model_written_evals/sycophancy/sycophancy_on_political_typology_quiz.yaml",
      "task_name": "sycophancy_on_political_typology_quiz",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "EleutherAI/sycophancy",
      "dataset_load_error": "Dataset scripts are no longer supported, but found sycophancy.py",
      "instance_build_error": "",
      "test_time_seconds": 0.62
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/moral_stories/moral_stories.yaml",
      "task_name": "moral_stories",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "demelin/moral_stories",
      "dataset_load_error": "Dataset scripts are no longer supported, but found moral_stories.py",
      "instance_build_error": "",
      "test_time_seconds": 0.69
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/mts_dialog/mts_dialog.yaml",
      "task_name": "mts_dialog",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "har1/MTS_Dialogue-Clinical_Note",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.43
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/mutual/mutual.yaml",
      "task_name": "mutual",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "EleutherAI/mutual",
      "dataset_load_error": "Dataset scripts are no longer supported, but found mutual.py",
      "instance_build_error": "",
      "test_time_seconds": 0.86
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/noreval/ncb/ncb.yaml",
      "task_name": "ncb",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hcfa/ncb",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 4.77
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/noreval/norec/norec_document/norec_document_p0.yaml",
      "task_name": "norec_document_p0",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "ltg/norec_document",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 8.16
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/noreval/norec/norec_document/norec_document_p1.yaml",
      "task_name": "norec_document_p1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "ltg/norec_document",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.7
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/noreval/norec/norec_document/norec_document_p2.yaml",
      "task_name": "norec_document_p2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "ltg/norec_document",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.63
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/noreval/norec/norec_document/norec_document_p3.yaml",
      "task_name": "norec_document_p3",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "ltg/norec_document",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.53
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/noreval/norec/norec_document/norec_document_p4.yaml",
      "task_name": "norec_document_p4",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "ltg/norec_document",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.54
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/noreval/norec/norec_sentence/norec_sentence_p0.yaml",
      "task_name": "norec_sentence_p0",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "ltg/norec_sentence",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.9
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/noreval/norec/norec_sentence/norec_sentence_p1.yaml",
      "task_name": "norec_sentence_p1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "ltg/norec_sentence",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.47
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/noreval/norec/norec_sentence/norec_sentence_p2.yaml",
      "task_name": "norec_sentence_p2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "ltg/norec_sentence",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.41
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/noreval/norec/norec_sentence/norec_sentence_p3.yaml",
      "task_name": "norec_sentence_p3",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "ltg/norec_sentence",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.48
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/noreval/norec/norec_sentence/norec_sentence_p4.yaml",
      "task_name": "norec_sentence_p4",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "ltg/norec_sentence",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.4
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/noreval/norrewrite-instruct/norrewrite_instruct.yaml",
      "task_name": "norrewrite_instruct",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "ltg/norrewrite-instruct",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 3.37
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/noreval/norsummarize-instruct/norsummarize_instruct.yaml",
      "task_name": "norsummarize_instruct",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "ltg/norsummarize-instruct",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.6
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/noticia/noticia.yaml",
      "task_name": "noticia",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "Iker/NoticIA",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.92
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/nq_open/nq_open.yaml",
      "task_name": "nq_open",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "nq_open",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 10.68
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/arc_multilingual/arc_ar.yaml",
      "task_name": "arc_ar",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.37
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/arc_multilingual/arc_bn.yaml",
      "task_name": "arc_bn",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.58
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/arc_multilingual/arc_ca.yaml",
      "task_name": "arc_ca",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.64
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/arc_multilingual/arc_da.yaml",
      "task_name": "arc_da",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.63
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/arc_multilingual/arc_de.yaml",
      "task_name": "arc_de",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.69
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/arc_multilingual/arc_es.yaml",
      "task_name": "arc_es",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 5.75
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/arc_multilingual/arc_eu.yaml",
      "task_name": "arc_eu",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.33
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/arc_multilingual/arc_fr.yaml",
      "task_name": "arc_fr",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.61
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/arc_multilingual/arc_gu.yaml",
      "task_name": "arc_gu",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.66
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/arc_multilingual/arc_hi.yaml",
      "task_name": "arc_hi",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.75
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/arc_multilingual/arc_hr.yaml",
      "task_name": "arc_hr",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.6
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/arc_multilingual/arc_hu.yaml",
      "task_name": "arc_hu",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.66
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/arc_multilingual/arc_hy.yaml",
      "task_name": "arc_hy",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.94
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/arc_multilingual/arc_id.yaml",
      "task_name": "arc_id",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.76
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/arc_multilingual/arc_it.yaml",
      "task_name": "arc_it",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.76
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/arc_multilingual/arc_kn.yaml",
      "task_name": "arc_kn",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.77
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/arc_multilingual/arc_ml.yaml",
      "task_name": "arc_ml",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.36
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/arc_multilingual/arc_mr.yaml",
      "task_name": "arc_mr",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.15
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/arc_multilingual/arc_ne.yaml",
      "task_name": "arc_ne",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.0
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/arc_multilingual/arc_nl.yaml",
      "task_name": "arc_nl",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.74
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/arc_multilingual/arc_pt.yaml",
      "task_name": "arc_pt",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.76
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/arc_multilingual/arc_ro.yaml",
      "task_name": "arc_ro",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.91
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/arc_multilingual/arc_ru.yaml",
      "task_name": "arc_ru",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.79
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/arc_multilingual/arc_sk.yaml",
      "task_name": "arc_sk",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.98
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/arc_multilingual/arc_sr.yaml",
      "task_name": "arc_sr",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.73
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/arc_multilingual/arc_sv.yaml",
      "task_name": "arc_sv",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.68
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/arc_multilingual/arc_ta.yaml",
      "task_name": "arc_ta",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.86
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/arc_multilingual/arc_te.yaml",
      "task_name": "arc_te",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.05
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/arc_multilingual/arc_uk.yaml",
      "task_name": "arc_uk",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.91
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/arc_multilingual/arc_vi.yaml",
      "task_name": "arc_vi",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.86
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/arc_multilingual/arc_zh.yaml",
      "task_name": "arc_zh",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.72
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/hellaswag_multilingual/hellaswag_ar.yaml",
      "task_name": "hellaswag_ar",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_hellaswag",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 7.87
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/hellaswag_multilingual/hellaswag_bn.yaml",
      "task_name": "hellaswag_bn",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_hellaswag",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 3.47
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/hellaswag_multilingual/hellaswag_ca.yaml",
      "task_name": "hellaswag_ca",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_hellaswag",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.61
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/hellaswag_multilingual/hellaswag_da.yaml",
      "task_name": "hellaswag_da",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_hellaswag",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.31
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/hellaswag_multilingual/hellaswag_de.yaml",
      "task_name": "hellaswag_de",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_hellaswag",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 3.83
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/hellaswag_multilingual/hellaswag_es.yaml",
      "task_name": "hellaswag_es",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_hellaswag",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 3.51
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/hellaswag_multilingual/hellaswag_eu.yaml",
      "task_name": "hellaswag_eu",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_hellaswag",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.71
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/hellaswag_multilingual/hellaswag_fr.yaml",
      "task_name": "hellaswag_fr",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_hellaswag",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.44
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/hellaswag_multilingual/hellaswag_gu.yaml",
      "task_name": "hellaswag_gu",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_hellaswag",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.75
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/hellaswag_multilingual/hellaswag_hi.yaml",
      "task_name": "hellaswag_hi",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_hellaswag",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 3.02
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/hellaswag_multilingual/hellaswag_hr.yaml",
      "task_name": "hellaswag_hr",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_hellaswag",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.67
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/hellaswag_multilingual/hellaswag_hu.yaml",
      "task_name": "hellaswag_hu",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_hellaswag",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 4.09
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/hellaswag_multilingual/hellaswag_hy.yaml",
      "task_name": "hellaswag_hy",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_hellaswag",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 3.07
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/hellaswag_multilingual/hellaswag_id.yaml",
      "task_name": "hellaswag_id",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_hellaswag",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.74
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/hellaswag_multilingual/hellaswag_it.yaml",
      "task_name": "hellaswag_it",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_hellaswag",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 6.09
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/hellaswag_multilingual/hellaswag_kn.yaml",
      "task_name": "hellaswag_kn",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_hellaswag",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.85
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/hellaswag_multilingual/hellaswag_ml.yaml",
      "task_name": "hellaswag_ml",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_hellaswag",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 6.81
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/hellaswag_multilingual/hellaswag_mr.yaml",
      "task_name": "hellaswag_mr",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_hellaswag",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.87
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/hellaswag_multilingual/hellaswag_ne.yaml",
      "task_name": "hellaswag_ne",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_hellaswag",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.83
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/hellaswag_multilingual/hellaswag_nl.yaml",
      "task_name": "hellaswag_nl",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_hellaswag",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 3.16
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/hellaswag_multilingual/hellaswag_pt.yaml",
      "task_name": "hellaswag_pt",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_hellaswag",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 5.26
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/hellaswag_multilingual/hellaswag_ro.yaml",
      "task_name": "hellaswag_ro",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_hellaswag",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 6.22
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/hellaswag_multilingual/hellaswag_ru.yaml",
      "task_name": "hellaswag_ru",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_hellaswag",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.84
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/hellaswag_multilingual/hellaswag_sk.yaml",
      "task_name": "hellaswag_sk",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_hellaswag",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.25
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/hellaswag_multilingual/hellaswag_sr.yaml",
      "task_name": "hellaswag_sr",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_hellaswag",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.52
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/hellaswag_multilingual/hellaswag_sv.yaml",
      "task_name": "hellaswag_sv",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_hellaswag",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.28
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/hellaswag_multilingual/hellaswag_ta.yaml",
      "task_name": "hellaswag_ta",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_hellaswag",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 3.11
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/hellaswag_multilingual/hellaswag_te.yaml",
      "task_name": "hellaswag_te",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_hellaswag",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 5.29
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/hellaswag_multilingual/hellaswag_uk.yaml",
      "task_name": "hellaswag_uk",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_hellaswag",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 3.06
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/hellaswag_multilingual/hellaswag_vi.yaml",
      "task_name": "hellaswag_vi",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_hellaswag",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 3.84
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_ar_mc1.yaml",
      "task_name": "truthfulqa_ar_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.78
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_ar_mc2.yaml",
      "task_name": "truthfulqa_ar_mc2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.1
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_bn_mc1.yaml",
      "task_name": "truthfulqa_bn_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.6
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_bn_mc2.yaml",
      "task_name": "truthfulqa_bn_mc2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.77
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_ca_mc1.yaml",
      "task_name": "truthfulqa_ca_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.38
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_ca_mc2.yaml",
      "task_name": "truthfulqa_ca_mc2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.76
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_da_mc1.yaml",
      "task_name": "truthfulqa_da_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.17
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_da_mc2.yaml",
      "task_name": "truthfulqa_da_mc2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.85
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_de_mc1.yaml",
      "task_name": "truthfulqa_de_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.67
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_de_mc2.yaml",
      "task_name": "truthfulqa_de_mc2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.8
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_es_mc1.yaml",
      "task_name": "truthfulqa_es_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.18
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_es_mc2.yaml",
      "task_name": "truthfulqa_es_mc2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.85
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_eu_mc1.yaml",
      "task_name": "truthfulqa_eu_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.08
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_eu_mc2.yaml",
      "task_name": "truthfulqa_eu_mc2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.77
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_fr_mc1.yaml",
      "task_name": "truthfulqa_fr_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.09
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_fr_mc2.yaml",
      "task_name": "truthfulqa_fr_mc2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.75
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_gu_mc1.yaml",
      "task_name": "truthfulqa_gu_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.32
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_gu_mc2.yaml",
      "task_name": "truthfulqa_gu_mc2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.06
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_hi_mc1.yaml",
      "task_name": "truthfulqa_hi_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.0
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_hi_mc2.yaml",
      "task_name": "truthfulqa_hi_mc2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.82
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_hr_mc1.yaml",
      "task_name": "truthfulqa_hr_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.19
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_hr_mc2.yaml",
      "task_name": "truthfulqa_hr_mc2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.87
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_hu_mc1.yaml",
      "task_name": "truthfulqa_hu_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.05
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_hu_mc2.yaml",
      "task_name": "truthfulqa_hu_mc2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.77
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_hy_mc1.yaml",
      "task_name": "truthfulqa_hy_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.19
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_hy_mc2.yaml",
      "task_name": "truthfulqa_hy_mc2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 4.33
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_id_mc1.yaml",
      "task_name": "truthfulqa_id_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.61
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_id_mc2.yaml",
      "task_name": "truthfulqa_id_mc2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.08
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_it_mc1.yaml",
      "task_name": "truthfulqa_it_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.05
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_it_mc2.yaml",
      "task_name": "truthfulqa_it_mc2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.29
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_kn_mc1.yaml",
      "task_name": "truthfulqa_kn_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.41
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_kn_mc2.yaml",
      "task_name": "truthfulqa_kn_mc2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.76
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_ml_mc1.yaml",
      "task_name": "truthfulqa_ml_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.11
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_ml_mc2.yaml",
      "task_name": "truthfulqa_ml_mc2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.76
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_mr_mc1.yaml",
      "task_name": "truthfulqa_mr_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.08
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_mr_mc2.yaml",
      "task_name": "truthfulqa_mr_mc2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.86
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_ne_mc1.yaml",
      "task_name": "truthfulqa_ne_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.06
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_ne_mc2.yaml",
      "task_name": "truthfulqa_ne_mc2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.8
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_nl_mc1.yaml",
      "task_name": "truthfulqa_nl_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 3.07
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_nl_mc2.yaml",
      "task_name": "truthfulqa_nl_mc2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.03
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_pt_mc1.yaml",
      "task_name": "truthfulqa_pt_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.06
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_pt_mc2.yaml",
      "task_name": "truthfulqa_pt_mc2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.93
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_ro_mc1.yaml",
      "task_name": "truthfulqa_ro_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.46
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_ro_mc2.yaml",
      "task_name": "truthfulqa_ro_mc2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.76
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_ru_mc1.yaml",
      "task_name": "truthfulqa_ru_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.34
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_ru_mc2.yaml",
      "task_name": "truthfulqa_ru_mc2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 4.27
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_sk_mc1.yaml",
      "task_name": "truthfulqa_sk_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.5
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_sk_mc2.yaml",
      "task_name": "truthfulqa_sk_mc2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.05
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_sr_mc1.yaml",
      "task_name": "truthfulqa_sr_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.14
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_sr_mc2.yaml",
      "task_name": "truthfulqa_sr_mc2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.84
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_sv_mc1.yaml",
      "task_name": "truthfulqa_sv_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.15
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_sv_mc2.yaml",
      "task_name": "truthfulqa_sv_mc2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.52
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_ta_mc1.yaml",
      "task_name": "truthfulqa_ta_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.29
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_ta_mc2.yaml",
      "task_name": "truthfulqa_ta_mc2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.92
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_te_mc1.yaml",
      "task_name": "truthfulqa_te_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.59
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_te_mc2.yaml",
      "task_name": "truthfulqa_te_mc2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.88
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_uk_mc1.yaml",
      "task_name": "truthfulqa_uk_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.73
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_uk_mc2.yaml",
      "task_name": "truthfulqa_uk_mc2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 11.88
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_vi_mc1.yaml",
      "task_name": "truthfulqa_vi_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.77
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_vi_mc2.yaml",
      "task_name": "truthfulqa_vi_mc2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.75
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_zh_mc1.yaml",
      "task_name": "truthfulqa_zh_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.32
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual/truthfulqa_zh_mc2.yaml",
      "task_name": "truthfulqa_zh_mc2",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "alexandrainst/m_truthfulqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.75
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/olaph/olaph.yaml",
      "task_name": "olaph",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "dmis-lab/MedLFQA",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.68
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/olaph/olaph_perplexity.yaml",
      "task_name": "olaph_perplexity",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "dmis-lab/MedLFQA",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 8.0
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/openbookqa/openbookqa.yaml",
      "task_name": "openbookqa",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "openbookqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.36
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/pile/pile_arxiv.yaml",
      "task_name": "pile_arxiv",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "EleutherAI/pile",
      "dataset_load_error": "Dataset scripts are no longer supported, but found pile.py",
      "instance_build_error": "",
      "test_time_seconds": 0.48
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/pile_10k/pile_10k.yaml",
      "task_name": "pile_10k",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "NeelNanda/pile-10k",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.2
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/piqa/piqa.yaml",
      "task_name": "piqa",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "baber/piqa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.87
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/polemo2/polemo2_in.yaml",
      "task_name": "polemo2_in",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "allegro/klej-polemo2-in",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.33
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/polemo2/polemo2_out.yaml",
      "task_name": "polemo2_out",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "allegro/klej-polemo2-out",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.0
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/portuguese_bench/assin_entailment.yaml",
      "task_name": "assin_entailment",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "nilc-nlp/assin",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.33
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/portuguese_bench/assin_paraphrase.yaml",
      "task_name": "assin_paraphrase",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "nilc-nlp/assin",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.5
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/prost/corypaik_prost.yaml",
      "task_name": "prost",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "corypaik/prost",
      "dataset_load_error": "Dataset scripts are no longer supported, but found prost.py",
      "instance_build_error": "",
      "test_time_seconds": 0.77
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/pubmedqa/pubmedqa.yaml",
      "task_name": "pubmedqa",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "bigbio/pubmed_qa",
      "dataset_load_error": "Dataset scripts are no longer supported, but found pubmed_qa.py",
      "instance_build_error": "",
      "test_time_seconds": 0.48
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/qa4mre/qa4mre_2011.yaml",
      "task_name": "qa4mre_2011",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "qa4mre",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 3.38
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/qa4mre/qa4mre_2012.yaml",
      "task_name": "qa4mre_2012",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "qa4mre",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 3.0
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/qa4mre/qa4mre_2013.yaml",
      "task_name": "qa4mre_2013",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "qa4mre",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.65
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/qasper/bool.yaml",
      "task_name": "qasper_bool",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "allenai/qasper",
      "dataset_load_error": "Dataset scripts are no longer supported, but found qasper.py",
      "instance_build_error": "",
      "test_time_seconds": 0.61
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/qasper/freeform.yaml",
      "task_name": "qasper_freeform",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "allenai/qasper",
      "dataset_load_error": "Dataset scripts are no longer supported, but found qasper.py",
      "instance_build_error": "",
      "test_time_seconds": 0.35
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/race/race.yaml",
      "task_name": "race",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "EleutherAI/race",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.29
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/realtoxicityprompts/realtoxicityprompts.yaml",
      "task_name": "realtoxicityprompts",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "allenai/real-toxicity-prompts",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.69
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/ruler/niah_single_1.yaml",
      "task_name": "niah_single_1",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "",
      "dataset_load_error": "list index out of range",
      "instance_build_error": "",
      "test_time_seconds": 0.0
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/sciq/sciq.yaml",
      "task_name": "sciq",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "sciq",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.45
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/score/agi_eval/non_greedy_robustness_agieval_aqua_rat.yaml",
      "task_name": "non_greedy_robustness_agieval_aqua_rat",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-aqua-rat",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.5
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/score/agi_eval/non_greedy_robustness_agieval_logiqa_en.yaml",
      "task_name": "non_greedy_robustness_agieval_logiqa_en",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-logiqa-en",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.57
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/score/agi_eval/non_greedy_robustness_agieval_lsat_rc.yaml",
      "task_name": "non_greedy_robustness_agieval_lsat_rc",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-lsat-rc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.55
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/score/agi_eval/non_greedy_robustness_agieval_lstat_ar.yaml",
      "task_name": "non_greedy_robustness_agieval_lsat_ar",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-lsat-ar",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.53
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/score/agi_eval/non_greedy_robustness_agieval_lstat_lr.yaml",
      "task_name": "non_greedy_robustness_agieval_lsat_lr",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-lsat-lr",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.56
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/score/agi_eval/non_greedy_robustness_agieval_sat_en.yaml",
      "task_name": "non_greedy_robustness_agieval_sat_en",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-sat-en",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.61
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/score/agi_eval/non_greedy_robustness_agieval_sat_math.yaml",
      "task_name": "non_greedy_robustness_agieval_sat_math",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-sat-math",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.72
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/score/agi_eval/option_order_robustness_agieval_aqua_rat.yaml",
      "task_name": "option_order_robustness_agieval_aqua_rat",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-aqua-rat",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.43
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/score/agi_eval/option_order_robustness_agieval_logiqa_en.yaml",
      "task_name": "option_order_robustness_agieval_logiqa_en",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-logiqa-en",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.53
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/score/agi_eval/option_order_robustness_agieval_lsat_ar.yaml",
      "task_name": "option_order_robustness_agieval_lsat_ar",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-lsat-ar",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.48
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/score/agi_eval/option_order_robustness_agieval_lsat_lr.yaml",
      "task_name": "option_order_robustness_agieval_lsat_lr",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-lsat-lr",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.48
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/score/agi_eval/option_order_robustness_agieval_lsat_rc.yaml",
      "task_name": "option_order_robustness_agieval_lsat_rc",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-lsat-rc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.53
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/score/agi_eval/option_order_robustness_agieval_sat_en.yaml",
      "task_name": "option_order_robustness_agieval_sat_en",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-sat-en",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.48
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/score/agi_eval/option_order_robustness_agieval_sat_math.yaml",
      "task_name": "option_order_robustness_agieval_sat_math",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-sat-math",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.46
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/score/agi_eval/prompt_robustness_agieval_aqua_rat.yaml",
      "task_name": "prompt_robustness_agieval_aqua_rat",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-aqua-rat",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.43
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/score/agi_eval/prompt_robustness_agieval_logiqa_en.yaml",
      "task_name": "prompt_robustness_agieval_logiqa_en",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-logiqa-en",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.49
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/score/agi_eval/prompt_robustness_agieval_lsat_rc.yaml",
      "task_name": "prompt_robustness_agieval_lsat_rc",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-lsat-rc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.54
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/score/agi_eval/prompt_robustness_agieval_lstat_ar.yaml",
      "task_name": "prompt_robustness_agieval_lsat_ar",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-lsat-ar",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.49
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/score/agi_eval/prompt_robustness_agieval_lstat_lr.yaml",
      "task_name": "prompt_robustness_agieval_lsat_lr",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-lsat-lr",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.5
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/score/agi_eval/prompt_robustness_agieval_sat_en.yaml",
      "task_name": "prompt_robustness_agieval_sat_en",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-sat-en",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.49
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/score/agi_eval/prompt_robustness_agieval_sat_math.yaml",
      "task_name": "prompt_robustness_agieval_sat_math",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "hails/agieval-sat-math",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.41
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/score/math/non_greedy_robustness_math_algebra.yaml",
      "task_name": "non_greedy_robustness_math_algebra",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "EleutherAI/hendrycks_math",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.52
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/score/math/prompt_robustness_math_algebra.yaml",
      "task_name": "prompt_robustness_math_algebra",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "EleutherAI/hendrycks_math",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.51
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/score/mmlu_pro/score_non_greedy_robustness_mmlu_pro.yaml",
      "task_name": "score_non_greedy_robustness_mmlu_pro",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "TIGER-Lab/MMLU-Pro",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.55
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/score/mmlu_pro/score_option_order_robustness_mmlu_pro.yaml",
      "task_name": "score_option_order_robustness_mmlu_pro",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "TIGER-Lab/MMLU-Pro",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.55
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/score/mmlu_pro/score_prompt_robustness_mmlu_pro.yaml",
      "task_name": "score_prompt_robustness_mmlu_pro",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "TIGER-Lab/MMLU-Pro",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.51
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/simple_cooccurrence_bias/simple_cooccurrence_bias.yaml",
      "task_name": "simple_cooccurrence_bias",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "oskarvanderwal/simple-cooccurrence-bias",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.59
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/simple_cooccurrence_bias/simple_cooccurrence_bias_gen.yaml",
      "task_name": "simple_cooccurrence_bias_gen",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "oskarvanderwal/simple-cooccurrence-bias",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.49
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/siqa/siqa.yaml",
      "task_name": "social_iqa",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "social_i_qa",
      "dataset_load_error": "Dataset scripts are no longer supported, but found social_i_qa.py",
      "instance_build_error": "",
      "test_time_seconds": 0.77
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/spanish_bench/cocoteros_es.yaml",
      "task_name": "cocoteros_es",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "gplsi/cocoteros",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.09
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/spanish_bench/copa_es.yaml",
      "task_name": "copa_es",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "BSC-LT/COPA-es",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.62
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/spanish_bench/escola.yaml",
      "task_name": "escola",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "nbel/EsCoLA",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.22
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/spanish_bench/openbookqa_es.yaml",
      "task_name": "openbookqa_es",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "BSC-LT/openbookqa-es",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.47
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/spanish_bench/paws_es_spanish_bench.yaml",
      "task_name": "paws_es_spanish_bench",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "paws-x",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 6.07
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/spanish_bench/wnli_es.yaml",
      "task_name": "wnli_es",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "PlanTL-GOB-ES/wnli-es",
      "dataset_load_error": "Dataset scripts are no longer supported, but found wnli-es.py",
      "instance_build_error": "",
      "test_time_seconds": 0.81
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/spanish_bench/xlsum_es.yaml",
      "task_name": "xlsum_es",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "csebuetnlp/xlsum",
      "dataset_load_error": "Dataset scripts are no longer supported, but found xlsum.py",
      "instance_build_error": "",
      "test_time_seconds": 0.58
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/spanish_bench/xnli_es_spanish_bench.yaml",
      "task_name": "xnli_es_spanish_bench",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "xnli",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 3.0
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/storycloze/storycloze_2016.yaml",
      "task_name": "storycloze_2016",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "story_cloze",
      "dataset_load_error": "Dataset scripts are no longer supported, but found story_cloze.py",
      "instance_build_error": "",
      "test_time_seconds": 0.83
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/storycloze/storycloze_2018.yaml",
      "task_name": "storycloze_2018",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "story_cloze",
      "dataset_load_error": "Dataset scripts are no longer supported, but found story_cloze.py",
      "instance_build_error": "",
      "test_time_seconds": 0.6
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/super_glue/boolq/default.yaml",
      "task_name": "boolq",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "super_glue",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.4
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/super_glue/boolq/seq2seq.yaml",
      "task_name": "boolq-seq2seq",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "super_glue",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.86
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/super_glue/boolq/t5-prompt.yaml",
      "task_name": "super_glue-boolq-t5-prompt",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "super_glue",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.86
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/super_glue/cb/default.yaml",
      "task_name": "cb",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "super_glue",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.87
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/super_glue/cb/t5-prompt.yaml",
      "task_name": "super_glue-cb-t5-prompt",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "super_glue",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.8
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/super_glue/copa/default.yaml",
      "task_name": "copa",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "super_glue",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.81
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/super_glue/copa/t5-prompt.yaml",
      "task_name": "super_glue-copa-t5-prompt",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "super_glue",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.75
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/super_glue/multirc/default.yaml",
      "task_name": "multirc",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "super_glue",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.96
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/super_glue/multirc/t5-prompt.yaml",
      "task_name": "super_glue-multirc-t5-prompt",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "super_glue",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.88
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/super_glue/record/default.yaml",
      "task_name": "record",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "super_glue",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.18
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/super_glue/record/t5-prompt.yaml",
      "task_name": "super_glue-record-t5-prompt",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "super_glue",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.95
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/super_glue/rte/default.yaml",
      "task_name": "sglue_rte",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "super_glue",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.07
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/super_glue/rte/t5-prompt.yaml",
      "task_name": "super_glue-rte-t5-prompt",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "super_glue",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.89
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/super_glue/wic/default.yaml",
      "task_name": "wic",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "super_glue",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.15
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/super_glue/wic/t5-prompt.yaml",
      "task_name": "super_glue-wic-t5-prompt",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "super_glue",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.92
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/super_glue/wsc/default.yaml",
      "task_name": "wsc",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "super_glue",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.91
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/super_glue/wsc/t5-prompt.yaml",
      "task_name": "super_glue-wsc-t5-prompt",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "super_glue",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.71
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/swag/swag.yaml",
      "task_name": "swag",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "swag",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.73
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/tinyBenchmarks/tinyArc.yaml",
      "task_name": "tinyArc",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "tinyBenchmarks/tinyAI2_arc",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.73
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/tinyBenchmarks/tinyGSM8k.yaml",
      "task_name": "tinyGSM8k",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "tinyBenchmarks/tinyGSM8k",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.18
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/tinyBenchmarks/tinyHellaswag.yaml",
      "task_name": "tinyHellaswag",
      "fully_working": false,
      "failure_stage": "instance_build",
      "config_load_error": "",
      "dataset_path": "tinyBenchmarks/tinyHellaswag",
      "dataset_load_error": "",
      "instance_build_error": "'query' is undefined",
      "test_time_seconds": 2.89
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/tinyBenchmarks/tinyMMLU.yaml",
      "task_name": "tinyMMLU",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "tinyBenchmarks/tinyMMLU",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.06
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/tinyBenchmarks/tinyTruthfulQA_mc1.yaml",
      "task_name": "tinyTruthfulQA_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "tinyBenchmarks/tinyTruthfulQA",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.4
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/tinyBenchmarks/tinyWinogrande.yaml",
      "task_name": "tinyWinogrande",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "tinyBenchmarks/tinyWinogrande",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.28
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/toxigen/toxigen.yaml",
      "task_name": "toxigen",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "skg/toxigen-data",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.54
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/translation/iwslt2017_ar-en.yaml",
      "task_name": "iwslt2017-ar-en",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "iwslt2017",
      "dataset_load_error": "Dataset scripts are no longer supported, but found iwslt2017.py",
      "instance_build_error": "",
      "test_time_seconds": 0.89
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/translation/iwslt2017_en-ar.yaml",
      "task_name": "iwslt2017-en-ar",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "iwslt2017",
      "dataset_load_error": "Dataset scripts are no longer supported, but found iwslt2017.py",
      "instance_build_error": "",
      "test_time_seconds": 0.62
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/translation/wmt14_en-fr.yaml",
      "task_name": "wmt14-en-fr",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "wmt14",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.86
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/translation/wmt14_fr-en.yaml",
      "task_name": "wmt14-fr-en",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "wmt14",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.83
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/translation/wmt16_de-en.yaml",
      "task_name": "wmt16-de-en",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "wmt16",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.67
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/translation/wmt16_en-de.yaml",
      "task_name": "wmt16-en-de",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "wmt16",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.93
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/translation/wmt16_en-ro.yaml",
      "task_name": "wmt16-en-ro",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "wmt16",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 6.01
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/translation/wmt16_ro-en.yaml",
      "task_name": "wmt16-ro-en",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "wmt16",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.28
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/triviaqa/default.yaml",
      "task_name": "triviaqa",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "trivia_qa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.18
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/truthfulqa/truthfulqa_gen.yaml",
      "task_name": "truthfulqa_gen",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "truthful_qa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.77
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/truthfulqa/truthfulqa_mc1.yaml",
      "task_name": "truthfulqa_mc1",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "truthful_qa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 2.0
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/unscramble/anagrams1.yaml",
      "task_name": "anagrams1",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "EleutherAI/unscramble",
      "dataset_load_error": "Dataset scripts are no longer supported, but found unscramble.py",
      "instance_build_error": "",
      "test_time_seconds": 0.74
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/unscramble/anagrams2.yaml",
      "task_name": "anagrams2",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "EleutherAI/unscramble",
      "dataset_load_error": "Dataset scripts are no longer supported, but found unscramble.py",
      "instance_build_error": "",
      "test_time_seconds": 0.5
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/unscramble/cycle_letters.yaml",
      "task_name": "cycle_letters",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "EleutherAI/unscramble",
      "dataset_load_error": "Dataset scripts are no longer supported, but found unscramble.py",
      "instance_build_error": "",
      "test_time_seconds": 0.37
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/unscramble/random_insertion.yaml",
      "task_name": "random_insertion",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "EleutherAI/unscramble",
      "dataset_load_error": "Dataset scripts are no longer supported, but found unscramble.py",
      "instance_build_error": "",
      "test_time_seconds": 0.34
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/unscramble/reversed_words.yaml",
      "task_name": "reversed_words",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "EleutherAI/unscramble",
      "dataset_load_error": "Dataset scripts are no longer supported, but found unscramble.py",
      "instance_build_error": "",
      "test_time_seconds": 0.35
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/webqs/webqs.yaml",
      "task_name": "webqs",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "web_questions",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.79
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/wikitext/wikitext.yaml",
      "task_name": "wikitext",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "EleutherAI/wikitext_document_level",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.89
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/winogender/winogender.yaml",
      "task_name": "winogender_all",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "oskarvanderwal/winogender",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.92
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/winogrande/default.yaml",
      "task_name": "winogrande",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "winogrande",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.42
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/wmt2016/ro_en-t5_prompt.yaml",
      "task_name": "wmt-ro-en-t5-prompt",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "wmt16",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 0.84
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/wsc273/default.yaml",
      "task_name": "wsc273",
      "fully_working": false,
      "failure_stage": "dataset_load",
      "config_load_error": "",
      "dataset_path": "winograd_wsc",
      "dataset_load_error": "Dataset scripts are no longer supported, but found winograd_wsc.py",
      "instance_build_error": "",
      "test_time_seconds": 0.86
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/xcopa/default_et.yaml",
      "task_name": "xcopa_et",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "xcopa",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.29
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/xnli_eu/xnli_eu.yaml",
      "task_name": "xnli_eu",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "HiTZ/xnli-eu",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.08
    },
    {
      "yaml_path": "/home/user/lm-evaluation-harness/lm_eval/tasks/xstorycloze/default_ar.yaml",
      "task_name": "xstorycloze_ar",
      "fully_working": true,
      "failure_stage": "none",
      "config_load_error": "",
      "dataset_path": "juletxara/xstory_cloze",
      "dataset_load_error": "",
      "instance_build_error": "",
      "test_time_seconds": 1.83
    }
  ],
  "total_test_time_seconds": 2045.8,
  "total_test_time_human": "0h 34m 5s"
}